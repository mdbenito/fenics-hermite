{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    " # Table of Contents\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\" id=\"toc-level0\"><li><span><a href=\"http://0.0.0.0:8888/notebooks/hermite/discrete_gradient.ipynb#Discrete-gradient-operators\" data-toc-modified-id=\"Discrete-gradient-operators-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Discrete gradient operators</a></span></li><li><span><a href=\"http://0.0.0.0:8888/notebooks/hermite/discrete_gradient.ipynb#Gradient-of-a-piecewise-linear-function\" data-toc-modified-id=\"Gradient-of-a-piecewise-linear-function-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Gradient of a piecewise linear function</a></span><ul class=\"toc-item\"><li><span><a href=\"http://0.0.0.0:8888/notebooks/hermite/discrete_gradient.ipynb#A-detour-using-projections\" data-toc-modified-id=\"A-detour-using-projections-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>A detour using projections</a></span><ul class=\"toc-item\"><li><span><a href=\"http://0.0.0.0:8888/notebooks/hermite/discrete_gradient.ipynb#How-does-the-assembly-of-the-projection-problem-work?\" data-toc-modified-id=\"How-does-the-assembly-of-the-projection-problem-work?-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>How does the assembly of the projection problem work?</a></span></li></ul></li><li><span><a href=\"http://0.0.0.0:8888/notebooks/hermite/discrete_gradient.ipynb#And-back\" data-toc-modified-id=\"And-back-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>And back</a></span></li><li><span><a href=\"http://0.0.0.0:8888/notebooks/hermite/discrete_gradient.ipynb#Using-the-PETSc-backend\" data-toc-modified-id=\"Using-the-PETSc-backend-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Using the PETSc backend</a></span></li></ul></li><li><span><a href=\"http://0.0.0.0:8888/notebooks/hermite/discrete_gradient.ipynb#Discrete-gradient-for-DKT\" data-toc-modified-id=\"Discrete-gradient-for-DKT-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Discrete gradient for DKT</a></span><ul class=\"toc-item\"><li><span><a href=\"http://0.0.0.0:8888/notebooks/hermite/discrete_gradient.ipynb#Tests\" data-toc-modified-id=\"Tests-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Tests</a></span></li><li><span><a href=\"http://0.0.0.0:8888/notebooks/hermite/discrete_gradient.ipynb#Iterating-over-facets\" data-toc-modified-id=\"Iterating-over-facets-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Iterating over facets</a></span></li><li><span><a href=\"http://0.0.0.0:8888/notebooks/hermite/discrete_gradient.ipynb#C++-implementation\" data-toc-modified-id=\"C++-implementation-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>C++ implementation</a></span></li><li><span><a href=\"http://0.0.0.0:8888/notebooks/hermite/discrete_gradient.ipynb#Tests\" data-toc-modified-id=\"Tests-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Tests</a></span></li></ul></li><li><span><a href=\"http://0.0.0.0:8888/notebooks/hermite/discrete_gradient.ipynb#Dump\" data-toc-modified-id=\"Dump-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Dump</a></span><ul class=\"toc-item\"><li><span><a href=\"http://0.0.0.0:8888/notebooks/hermite/discrete_gradient.ipynb#A-quick-retesting-of-the-assembly-of-the-matrix-for-$\\nabla_h$\" data-toc-modified-id=\"A-quick-retesting-of-the-assembly-of-the-matrix-for-$\\nabla_h$-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>A quick retesting of the assembly of the matrix for $\\nabla_h$</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete gradient operators\n",
    "\n",
    "This notebook collects my progress towards the definition of the operator\n",
    "\n",
    "$$\\nabla_h:W_h \\rightarrow \\Theta_h$$\n",
    "\n",
    "as defined in (Bartels, 2015) ยง8.2.1. We proceed as follows:\n",
    "\n",
    "1. Computation of (global) exact gradient for CG1 functions, i.e. as an operator from CG1 into CG0. This serves to test the idea with a simpler operator. See [Gradient of a piecewise linear function](#Gradient-of-a-piecewise-linear-function).\n",
    "2. Computation of (global) gradient $\\nabla_h$ for DKT functions, i.e. as an operator from DKT into CG2^2. See [Discrete gradient for DKT](#Discrete-gradient-for-DKT). This implementation basically works, but being pure python is extremely slow. A small improvement might be achieved by [Iterating over facets](#Iterating-over-facets). However we are building a huge matrix and multiplying the system matrix by it and its transpose, which is very slow.\n",
    "3. Implementation in C++ of the assembly process. This requires a new assembler for each problem or at least for each family of related ones, but should be pretty straightfoward if one uses FEniCS compiled forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## The usual boilerplate...\n",
    "\n",
    "from itertools import chain\n",
    "from dolfin import *\n",
    "from petsc4py import PETSc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "%matplotlib inline\n",
    "\n",
    "np.set_printoptions(precision=3, linewidth=150)\n",
    "\n",
    "def __nbinit__():\n",
    "    global __all__\n",
    "    __all__ = ['compute_discrete_gradient_dense', \n",
    "               'compute_discrete_gradient',\n",
    "               'compute_dkt_gradient']\n",
    "    global parameters\n",
    "    # instruct FFC to produce the code for evaluate_basis_derivatives()\n",
    "    parameters[\"form_compiler\"][\"no-evaluate_basis_derivatives\"] = False\n",
    "\n",
    "__nbinit__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient of a piecewise linear function\n",
    "\n",
    "Let $u_{h}$ be a piecewise linear function over a mesh $\\mathcal{T}_{h}$, that is $u_{h} \\in V_{h} =\\operatorname{span} \\{ \\phi^i_{h} \\}$, where $V_{h}$ is a CG1 finite element space and $\\phi^i$ are locally in $P_{1} (T)$ for some simplex $T \\in \\mathcal{T}_{h}$. Then $u_{h}'$ is piecewise constant, i.e. $u'_{h} \\in V_{h}' =\\operatorname{span} \\{ \\psi _{h}^i \\}$, a DG0 space over $\\mathcal{T}_{h}$, where $\\psi^i_{h}$ are constant over simplices. Denote by $U = (U_{i})$ and $U' = (U'_{i})$ their vectors of coefficients wrt. the standard bases of nodal functions over $V_{h}$ and $V_{h}'$ respectively. We want to compute a matrix $M$ for the gradient operator\n",
    "\n",
    "\\\\[ \\nabla _{h} : V_{h} \\rightarrow V_{h}' \\\\]\n",
    "\n",
    "such that $(M U)_{j} = U'_{j}$.\n",
    "Since $u'_{h}$ is not in the same space as $u_{h}$, one way of computing the coefficients $U'$ in FEniCS is to `df.project(df.grad(u), V)` as mentioned above. But here we manually compute the transformation without solving a linear system. Note that if we were only interested in evaluation of $u_{h}'$, because $u_{h} = \\sum_{i} U_{i} \\phi^i_{h}$ and by linearity $u_{h}' = \\sum_{i} U_{i} (\\phi_{h}^i)'$. We could then use `df.evaluate_basis_derivatives()` to evaluate the $\\phi _{i}'$ and we would have finished.\n",
    "\n",
    "But again, we are interested in obtaining the new coefficients $U_{i}'$ such that\n",
    "\\\\[ u' = \\sum_{i} U'_{i} \\psi _{i} . \\\\]\n",
    "\n",
    "We assemble $M$ cell by cell as follows, assuming that both $V$ and $V'$ are defined over the same mesh $\\mathcal{T}_{h}$ with $N$ cells.\n",
    "\n",
    "\n",
    "  * First, for any given cell $T$ compute the mapping $L_{T}$ of the dofs in $V_{|T}$ to the dofs in $V'_{|T}$.\n",
    "  * Then apply this mapping to the components of $U_{|T}$ to obtain the coefficients $U'_{|T}$.\n",
    "\n",
    "To explain the idea, consider first a 1D mesh for simplicity and assume that we have an index $i \\in [N]$ over the cells of the mesh such that:\n",
    "\n",
    "  * $U_{i}, U_{i + 1}$ are the coefficients for the dofs in $V$ over cell $i$.\n",
    "  * $U_{i}'$ is the coefficient for the only dof in $V'$ (the constant 1) over cell $i$.\n",
    "\n",
    "This assumption is actually a description of the mapping $L_{T}$ mentioned above for each interval. Then we only need to compute $(\\phi^i_{h})'$ (this is not a typo, we are actually computing the derivative) for each $i$ to obtain the new vector of coefficients $U_{|T}'$:\n",
    "\n",
    "\\\\[ M_{T} U_{|T} = \\left(\\begin{array}{ccccc}\n",
    "     \\phi _{h}^{0\\prime} & \\phi _{h}^{1\\prime} &  &  &  \\\\\n",
    "     & \\phi _{h}^{1\\prime} & \\phi _{h}^{2\\prime} &  &  \\\\\n",
    "     &  & \\phi _{h}^{2\\prime} & \\phi _{h}^{3\\prime} &  \\\\\n",
    "     &  &  & \\phi _{h}^{3\\prime} & \\phi _{h}^{4\\prime}\n",
    "   \\end{array}\\right) \\left(\\begin{array}{c}\n",
    "     U_{0}\\\\\n",
    "     U_{1}\\\\\n",
    "     U_{2}\\\\\n",
    "     U_{3}\\\\\n",
    "     U_{4}\n",
    "   \\end{array}\\right) = \\left(\\begin{array}{c}\n",
    "     U_{0}'\\\\\n",
    "     U_{1}'\\\\\n",
    "     U_{2}'\\\\\n",
    "     U_{3}'\n",
    "   \\end{array}\\right) . \\\\]\n",
    "   \n",
    "Note that in this particular instance and with the assumptions made, we obtain a band matrix and computing $U'$ is actually a matter of computing the dot product of a \u0010subset\u0011 of the vector of constants $(\\phi _{h}^{0\\prime}, ..., \\phi _{h}^{3\\prime})$ with $U$.\n",
    "\n",
    "However in other situations, e.g. in higher dimensions, the ordering of the nodes will not be as convenient. We proceed then more generally as follows, now for the global matrix. Let $i$ run over all cell indices and let $\\iota _{i}$ be its local-to-global mapping for $V$ and $\\iota'_{i}$ for $V'$. Initialise the global matrix $M$ of the discrete gradient to zero. At each step of the construction of $M$ we edit row $r = \\iota _{i}' (0)$, i.e. the row whose product by $U$ will return the coefficient for the dof in $V'$ corresponding to cell $i$.\n",
    "\n",
    "Now, for every vertex $z_{j}, j \\in [d]$ in cell $i$ compute the value of the derivative of the global dof $\\phi _{h}^{\\iota _{r} (j)}$ and set check this, I'm not sure after so many changes in notation\n",
    "\n",
    "\\\\[ M_{rj} = \\phi _{h}^{\\iota _{r} (j)\\prime} (z_{j}), j \\in [d] . \\\\]\n",
    "\n",
    "Here is a straightforward implementation of this procedure using AIJ sparse matrices in PETSc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discrete_gradient_operator_dense(V, Vp):\n",
    "    \"\"\" Build dense matrix of the discrete gradient\n",
    "    operator between V and Vp.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "        V: Source FunctionSpace of type CG1.\n",
    "        Vp: Target (Vector)FunctionSpace of type DG0.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        Dh: np.ndarray of shape (Vp.dim(), V.dim()) such that,\n",
    "            for u in V:\n",
    "        \n",
    "                np.dot(Dh, u.vector().array())\n",
    "              \n",
    "            yields the vector of coefficients of the discrete\n",
    "            gradient of u in Vp.\n",
    "    \"\"\"\n",
    "    gdim = V.mesh().geometry().dim()\n",
    "    assert V.ufl_element().shortstr()[:3] == 'CG1', \"Need a CG1 source space\"\n",
    "    assert Vp.ufl_element().shortstr()[:14] == 'Vector<%d x DG0' % gdim or\\\n",
    "           Vp.ufl_element().shortstr()[:3] == 'DG0',\\\n",
    "           \"I need a DG0 (Vector)FunctionSpace of the right number of components\"\n",
    "    assert Vp.mesh().num_cells() * gdim == Vp.dim(),\\\n",
    "           \"FunctionSpace dimensions don't match.\"\n",
    "    assert Vp.mesh().num_cells() == V.mesh().num_cells(),\\\n",
    "           \"Meshes don't match\"\n",
    "\n",
    "    dm = V.dofmap()\n",
    "    dmp = Vp.dofmap()\n",
    "    e = V.element()\n",
    "    coords = V.tabulate_dof_coordinates().reshape((-1, gdim))\n",
    "    coordsp = Vp.tabulate_dof_coordinates().reshape((-1, gdim))\n",
    "\n",
    "    Dh = np.zeros((Vp.dim(), V.dim()))\n",
    "    warning(\"Assuming all cells have the same number of dofs\")\n",
    "    vals = np.zeros(dm.num_element_dofs(0)*gdim)\n",
    "    for cell_id in range(Vp.mesh().num_cells()):\n",
    "        rows = dmp.cell_dofs(cell_id)\n",
    "        columns = dm.cell_dofs(cell_id)\n",
    "        dof_coords = coords[columns,:]\n",
    "        point = dof_coords[0] # Any point in the (closure of the) cell will do.\n",
    "        e.evaluate_basis_derivatives_all(1, vals, point, dof_coords, 1)\n",
    "        #print(\"rows=%s, columns=%s, point=%s, dof_coords=%s, vals=%s\" %\n",
    "        #     (rows, columns, point, dof_coords, vals.round(2)))\n",
    "        # Reshape [df_1/dx, df_1/dy, df_2/dx, df_2/dy, ...] into\n",
    "        # [[df_1/dx, df_2/dx, ...],[df_1/dy, df_2/dy, ...]] :\n",
    "        #Dh[FIXTHIS:\"rows, columns\"] = vals.reshape((-1, gdim)).T.copy()\n",
    "        for offset in range(gdim):\n",
    "            Dh[rows[offset], columns] = vals[offset::gdim].copy()\n",
    "\n",
    "    return Dh\n",
    "\n",
    "def compute_discrete_gradient_dense(V, Vp, u):\n",
    "    \"\"\" Returns the discrete gradient of u.\n",
    "    NOTE: this implementation uses dense matrices internally.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "        V: Source FunctionSpace of type CG1. (redundant...)\n",
    "        Vp: Target (Vector)FunctionSpace of type DG0.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        A Function in Vp interpolating the gradient of u.\n",
    "    \"\"\"\n",
    "\n",
    "    Dh = discrete_gradient_operator_dense(V, Vp)\n",
    "    up = Function(Vp)\n",
    "    up.vector().set_local(np.dot(Dh, u.vector().array()))\n",
    "    up.vector().apply('insert')\n",
    "\n",
    "    return up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test the previous code we start with an expression whose derivative we can compute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEFT, RIGHT = -pi, 2*pi\n",
    "mesh = IntervalMesh(100, LEFT, RIGHT)\n",
    "V = FunctionSpace(mesh, \"CG\", 1)\n",
    "Vp = FunctionSpace(mesh, \"DG\", 0)\n",
    "\n",
    "#x = SpatialCoordinate(mesh)\n",
    "#f = sin(x[0])\n",
    "#u = project(f, V)\n",
    "u = interpolate(Expression(\"sin(x[0])\", element=V.ufl_element()), V)\n",
    "dg_u = compute_discrete_gradient_dense(V, Vp, u)\n",
    "\n",
    "# Test\n",
    "#up = project(f.dx(0), Vp)\n",
    "up = interpolate(Expression(\"cos(x[0])\", element=Vp.ufl_element()), Vp)\n",
    "print(\"Error: %e\" % (dg_u.vector() - up.vector()).norm('linf'))\n",
    "plot(up)\n",
    "plot(dg_u)\n",
    "_ = pl.xlim((LEFT*1.1, RIGHT*1.1)), pl.ylim((-1.1,1.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A detour using projections\n",
    "\n",
    "The following method computes the solution via a projection onto DG0 but instead of letting FEniCS assemble the mass matrix, we make use of the fact that it is diagonal with entries given by the cell volume. Then it is trivial to invert it and multiply the rhs of the projection problem. The idea and code are taken from [this question on FEniCS Q&A](https://fenicsproject.org/qa/12634/gradient-of-a-cg-order-one-element?show=12646#a12646)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mesh = UnitCubeMesh(10, 10, 10)\n",
    "W = FunctionSpace(mesh, 'CG', 1)\n",
    "Q = VectorFunctionSpace(mesh, 'DG', 0)\n",
    "p, q = TrialFunction(Q), TestFunction(Q)\n",
    "\n",
    "f = Function(W)\n",
    "f_vec = f.vector()\n",
    "f_vec.set_local(np.random.rand(f_vec.local_size()))\n",
    "f_vec.apply('insert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first compute the gradient via a projection. **NOTE** that the computation for the form $L$ will have to interpolate $f$, which is in \"W\", into the space $T$ where the test function $q$ lives in order to compute the integral. [WHY?]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "L = inner(nabla_grad(f), q)*dx\n",
    "M = assemble(inner(p, q)*dx)\n",
    "b = assemble(L)\n",
    "\n",
    "grad_f0 = Function(Q)\n",
    "x0 = grad_f0.vector()\n",
    "_ = solve(M, x0, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MiroK's idea (which amounts to building our matrix $D_h$) in the forum is to \"assemble the action of Minv onto the rhs - no mass matrix needed\". The improvement in speed is huge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "MinvL = (1/CellVolume(Q.mesh()))*inner(nabla_grad(f), q)*dx\n",
    "x = assemble(MinvL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grad_f = Function(Q, x)\n",
    "print(\"Error: %e\" % (x - x0).norm('linf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the first approach we compute $M$, then solve $M x = b$ and in the second we compute $M^{-1}$ directly to obtain $x = M^{-1} b$.\n",
    "\n",
    "### How does the assembly of the projection problem work?\n",
    "\n",
    "Write the compiled form to a file and check `dgrad_cell_integral_0_otherwise::tabulate_tensor()` for the implementation. Recall that `u.dx()` in the form is the derivative of a terminal node in UFL so it is the responsibility of FFC to fill that. How does this work again?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ffc\n",
    "with open(\"/tmp/dgrad.h\", \"wt\") as fd:\n",
    "    out = ffc.compile_form(MinvL, prefix=\"dgrad\")\n",
    "    fd.write(out[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And back\n",
    "\n",
    "We can use our function instead. This will be much slower and also use a lot of memory because of the dense matrix we are using. The error is different, though. **Could this be significant?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dg_f = compute_discrete_gradient_dense(W, Q, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Error: %e\" % (dg_f.vector() - grad_f0.vector()).norm('linf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the PETSc backend\n",
    "\n",
    "By using PETSc We can achieve a (back-of-the-napkin) speed improvement of up to 50 times wrt. the implementation with dense matrices and stay in the same range of running times than `project()`. We can only test for manageable matrix sizes but we can now of course handle bigger matrices. The speedup depends greatly on the size of the matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discrete_gradient_operator(V, Vp):\n",
    "    \"\"\" Build sparse PETSc matrix of the discrete gradient\n",
    "    operator between V and Vp.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "        V: Source FunctionSpace of type CG1.\n",
    "        Vp: Target (Vector)FunctionSpace of type DG0.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        Dh: PETSc sparse matrix such that for u in V,\n",
    "              Dh * u.vector().array()\n",
    "            yields the vector of coefficients of the discrete\n",
    "            gradient of u in Vp.\n",
    "    \"\"\"   \n",
    "    gdim = V.mesh().geometry().dim()\n",
    "    assert V.ufl_element().shortstr()[:3] == 'CG1',\\\n",
    "           \"Need a CG1 source space\"\n",
    "    assert Vp.ufl_element().shortstr()[:14] == 'Vector<%d x DG0' % gdim or\\\n",
    "           Vp.ufl_element().shortstr()[:3] == 'DG0',\\\n",
    "           \"I need a DG0 (Vector)FunctionSpace of the right number of components\"\n",
    "    assert Vp.mesh().num_cells() * gdim == Vp.dim(),\\\n",
    "           \"FunctionSpace dimensions don't match\"\n",
    "    assert Vp.mesh().num_cells() == V.mesh().num_cells(),\\\n",
    "           \"Number of cells in the meshes don't match\"\n",
    "\n",
    "    dm = V.dofmap()\n",
    "    dmp = Vp.dofmap()\n",
    "    e = V.element()\n",
    "    coords = V.tabulate_dof_coordinates().reshape((-1, gdim))\n",
    "    coordsp = Vp.tabulate_dof_coordinates().reshape((-1, gdim))\n",
    "\n",
    "    Dh = PETSc.Mat()\n",
    "    Dh.create(PETSc.COMM_WORLD)\n",
    "    Dh.setSizes([Vp.dim(), V.dim()])\n",
    "    Dh.setType(\"aij\")\n",
    "    Dh.setUp()\n",
    "    warning(\"Assuming all cells have the same number of dofs\")\n",
    "    vals = np.zeros(dm.num_element_dofs(0)*gdim)\n",
    "    # TODO: check this for parallel operation...\n",
    "    istart, iend = Dh.getOwnershipRange()\n",
    "    print (\"This process owns the range %d - %d\" % (istart, iend))\n",
    "    for cell_id in range(Vp.mesh().num_cells()):\n",
    "        rows = dmp.cell_dofs(cell_id)\n",
    "        rows = rows[(rows>=istart) & (rows < iend)]\n",
    "        columns = dm.cell_dofs(cell_id)\n",
    "        dof_coords = coords[columns,:]\n",
    "        point = dof_coords[0] # Any point in the (closure of the) cell will do.\n",
    "        e.evaluate_basis_derivatives_all(1, vals, point, dof_coords, 1)\n",
    "        #print(\"rows=%s, columns=%s, point=%s, dof_coords=%s, vals=%s\" %\n",
    "        #     (rows, columns, point, dof_coords, vals.round(2)))\n",
    "        # Reshape [df_1/dx, df_1/dy, df_2/dx, df_2/dy, ...] into\n",
    "        # [[df_1/dx, df_2/dx, ...],[df_1/dy, df_2/dy, ...]] :\n",
    "        # NOTE: the copy() is necessary!\n",
    "        Dh.setValues(rows, columns, vals.reshape((-1, gdim)).T.copy())\n",
    "    Dh.assemble()\n",
    "    return Dh\n",
    "\n",
    "def compute_discrete_gradient(V, Vp, u):\n",
    "    \"\"\" Returns the discrete gradient of u.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "        V: Source FunctionSpace of type CG1. (redundant...)\n",
    "        Vp: Target (Vector)FunctionSpace of type DG0.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        A Function in Vp interpolating the gradient of u.\n",
    "    \"\"\"\n",
    "    Dh = discrete_gradient_operator(V, Vp)\n",
    "    du = Function(Vp)\n",
    "    petsc_u = as_backend_type(u.vector()).vec()\n",
    "    petsc_du = as_backend_type(du.vector()).vec()\n",
    "    \n",
    "    Dh.mult(petsc_u, petsc_du)\n",
    "    du.vector().apply('insert')\n",
    "\n",
    "    return du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEFT, RIGHT = -pi, 2*pi\n",
    "mesh = IntervalMesh(1000, LEFT, RIGHT)\n",
    "V = FunctionSpace(mesh, \"CG\", 1)\n",
    "Vp = FunctionSpace(mesh, \"DG\", 0)\n",
    "\n",
    "#Dh = discrete_gradient_operator(V, Vp)\n",
    "u = interpolate(Expression(\"sin(x[0])\", degree=1), V)\n",
    "du0 = interpolate(Expression(\"cos(x[0])\", degree=0), Vp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "du = compute_discrete_gradient(V, Vp, u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Error: %e\" % (du.vector() - du0.vector()).norm('linf'))\n",
    "_ = plot(du)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is faster than the solution with dense matrices but obviously still much slower (a little below 40 times) than building $M^{-1}$ directly because of the hideously slow python loops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dgrad_f = compute_discrete_gradient(W, Q, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Error: %e\" % (dgrad_f.vector() - grad_f0.vector()).norm('linf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete gradient for DKT\n",
    "\n",
    "We proceed with the implementation of\n",
    "\n",
    "$$\\nabla_h:W_h \\rightarrow \\Theta_h$$\n",
    "\n",
    "for Functions over the whole space. Recall that the operator $\\nabla_h$ is given cell-wise by the matrix\n",
    "\n",
    "\\begin{equation}\n",
    "  M_T = \\left(\\begin{array}{cccccc}\n",
    "    0 & I_2 & 0 & 0 & 0 & 0\\\\\n",
    "    0 & 0 & 0 & I_2 & 0 & 0\\\\\n",
    "    0 & 0 & 0 & 0 & 0 & I_2\\\\\n",
    "    0 & 0 & \\tilde{t}_{S_1} & \\tilde{T}_{S_1} & - \\tilde{t}_{S_1} &\n",
    "    \\tilde{T}_{S_1}\\\\\n",
    "    \\tilde{t}_{S_2} & \\tilde{T}_{S_2} & 0 & 0 & - \\tilde{t}_{S_2} &\n",
    "    \\tilde{T}_{S_2}\\\\\n",
    "    \\tilde{t}_{S_3} & \\tilde{T}_{S_3} & - \\tilde{t}_{S_3} & \\tilde{T}_{S_3} &\n",
    "    0 & 0\n",
    "  \\end{array}\\right) \\in \\mathbb{R}^{12 \\times 9}, \\quad \\tilde{T}_{S_l} =\n",
    "  \\tfrac{- 3}{4} t_{S_l} t_{S_l}^{\\top} + \\tfrac{1}{2} I_2, \n",
    "  \\quad \\tilde{t}_{S_l} = \\tfrac{- 3}{2\n",
    "  | S_l |} t_{S_l}\n",
    "\\end{equation}\n",
    "\n",
    "Since part of $M_T$ won't change, we compute it in two steps: an initialization step, which sets the fixed entries and a per-cell step, where we set the entries for each cell.\n",
    "\n",
    "**NOTE:** Because we are not using a hierarchical basis, the computations in (Bartels, 2015) Ch.8 don't apply verbatim and our $\\tilde{T}_{S_l}$ differs by a constant $+ \\tfrac{1}{2} I_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DKTCellGradient(object):\n",
    "    \"\"\" Computes the transformation matrix for $\\nabla_h$ over one cell.\n",
    "    \n",
    "    This assumes that the dofs of the \"source\" cell (in DKT) are ordered as\n",
    "    \n",
    "         w1, w1_x, w1_y, w2, w2_x, w2_y, w3, w3_x, w3_y,\n",
    "         \n",
    "    i.e. point evaluation, partial derivatives for each vertex in turn, and\n",
    "    those of the \"target\" cell (in P_2^2) as\n",
    "    \n",
    "        t1_0, t1_1, t2_0, t2_1, t3_0, t3_1,\n",
    "        ts1_0, ts1_1, ts2_0, ts2_1, ts3_0, ts3_1\n",
    "        \n",
    "    i.e. values at the three vertices, then values at the the midpoints of\n",
    "    the opposite sides.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\" Initialize the local cell gradient matrix.\n",
    "        The matrix is 12x9  (target's local space_dimension() x source's \n",
    "        local space_dimension().\"\"\"\n",
    "\n",
    "        self.M = np.zeros((12, 9), dtype=np.float)\n",
    "        rows = [[0,1], [2,3], [4,5], [6,7], [8,9], [10,11]]\n",
    "        # Hack to make fancy indexing with lists work:\n",
    "        # if one of rows[i], self.cols[j] has shape 2,1, broadcasting will actually\n",
    "        # compute the outer product of rows[i] and cols[j], instead of a \"zip\"\n",
    "        # of sorts.\n",
    "        self.rows = list(map(lambda x: np.array(x).reshape(2,1), rows))\n",
    "        self.cols = [0, [1,2], 3, [4,5], 6, [7,8]]\n",
    "        #self.cols = list(map(lambda x: np.array(x).reshape((-1,1)), self.cols))\n",
    "        # TODO: Note that storing these copies of the identity is silly\n",
    "        # When we apply the operator, it would make more sense to just copy\n",
    "        # the partial derivative dofs (which is what these first 3 multiplications \n",
    "        # with the identity do), then multiply with the lower part of M...\n",
    "        # (But we still need these here if we insist on building a global matrix\n",
    "        # which acts on all dofs of the space simultaneously)\n",
    "        self.M[self.rows[0], self.cols[1]] = np.eye(2)\n",
    "        self.M[self.rows[1], self.cols[3]] = np.eye(2)\n",
    "        self.M[self.rows[2], self.cols[5]] = np.eye(2)\n",
    "\n",
    "        # We are only defined for triangles (to do: assert this)\n",
    "        # tdim = 2, num_vertices = 3\n",
    "\n",
    "        # Init temporaries\n",
    "        self.tt = np.empty((3, 2))\n",
    "        self.TT = np.empty((3, 2, 2))\n",
    "        self.ss = np.empty(3)\n",
    "\n",
    "    def update(self, cell:Cell):\n",
    "        \"\"\"Update the mutating entries of M for the given cell.\"\"\"\n",
    "        cc = cell.get_vertex_coordinates().reshape(-1, 2)\n",
    "        self.tt[0] = (cc[2] - cc[1])\n",
    "        self.tt[1] = -(cc[0] - cc[2])  # Magic: why was this minus sign here again?\n",
    "        self.tt[2] = (cc[1] - cc[0])\n",
    "        for i in range(3):\n",
    "            self.ss[i] = np.linalg.norm(self.tt[i], ord=2)\n",
    "            self.tt[i] /= self.ss[i]\n",
    "            self.TT[i] = -3./4 * np.outer(self.tt[i], self.tt[i])\n",
    "            # HERE! if we don't use the hierarchical basis we need to add this, see...\n",
    "            self.TT[i] += 0.5*np.eye(2)\n",
    "            self.tt[i] *= -3./(2*self.ss[i])\n",
    "\n",
    "        self.M[self.rows[3], self.cols[2]] = self.tt[0].copy().reshape(2,1)\n",
    "        self.M[self.rows[3], self.cols[3]] = self.TT[0].copy()\n",
    "        self.M[self.rows[3], self.cols[4]] = -self.tt[0].copy().reshape(2,1)\n",
    "        self.M[self.rows[3], self.cols[5]] = self.TT[0].copy()\n",
    "\n",
    "        self.M[self.rows[4], self.cols[0]] = self.tt[1].copy().reshape(2,1)\n",
    "        self.M[self.rows[4], self.cols[1]] = self.TT[1].copy()\n",
    "        self.M[self.rows[4], self.cols[4]] = -self.tt[1].copy().reshape(2,1)\n",
    "        self.M[self.rows[4], self.cols[5]] = self.TT[1].copy()\n",
    "\n",
    "        self.M[self.rows[5], self.cols[0]] = self.tt[2].copy().reshape(2,1)\n",
    "        self.M[self.rows[5], self.cols[1]] = self.TT[2].copy()\n",
    "        self.M[self.rows[5], self.cols[2]] = -self.tt[2].copy().reshape(2,1)\n",
    "        self.M[self.rows[5], self.cols[3]] = self.TT[2].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now aggregate the previous cell-wise operator into one acting over discrete functions over the whole space.\n",
    "\n",
    "**FIXME:** the following code is very inefficient:\n",
    "\n",
    "1. There is no need to create and multiply by the first 6 rows of the cell matrix. If we didn't create a \"global matrix\", i.e. acting on whole vectors of coefficients, but instead proceeded cell by cell this would be faster (albeit at the cost of a python loop, which might prove to be even worse)\n",
    "2. When building such potentially large sparse matrices one should preinit the sparsity pattern.\n",
    "3. It should be possible to loop through all facets instead of cells. This should reduce computing time because of the fewer updates to the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     43
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dkt_gradient_operator(V, Vp):\n",
    "    \"\"\" Build sparse PETSc matrix of the discrete gradient\n",
    "    operator between V and Vp.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "        V: Source FunctionSpace of type DKT.\n",
    "        Vp: Target (Vector)FunctionSpace of type P2\n",
    "            (WITHOUT a hierarchical basis)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        Dh: PETSc sparse matrix such that for u in V,\n",
    "              Dh * u.vector().array()\n",
    "            yields the vector of coefficients of the discrete\n",
    "            gradient of u in Vp.\n",
    "    \"\"\"   \n",
    "    gdim = V.mesh().geometry().dim()\n",
    "    assert V.ufl_element().shortstr()[:3] == 'DKT',\\\n",
    "           \"I need a DKT source space\"\n",
    "    assert Vp.ufl_element().shortstr()[:14] == 'Vector<%d x CG2' % gdim,\\\n",
    "           \"I need a CG2 VectorFunctionSpace with %d components\" % gdim\n",
    "    assert Vp.mesh().num_cells() == V.mesh().num_cells(),\\\n",
    "           \"Number of cells in the meshes don't match\"\n",
    "    dm = V.dofmap()\n",
    "    dmp1, dmp2 = Vp.sub(0).dofmap(), Vp.sub(1).dofmap()\n",
    "    e = V.element()\n",
    "    ep = Vp.element()\n",
    "    tdim = e.topological_dimension()\n",
    "    assert tdim == 2, \"I need topological dimension 2 (was %d)\" % tdim\n",
    "    coords = V.tabulate_dof_coordinates().reshape((-1, tdim))\n",
    "\n",
    "    Dh = PETSc.Mat()\n",
    "    Dh.create(PETSc.COMM_WORLD)\n",
    "    Dh.setSizes([Vp.dim(), V.dim()])\n",
    "    Dh.setType(\"aij\")\n",
    "    Dh.setUp()\n",
    "    # In order not to assume the following it would be enough\n",
    "    # to create the array inside the loop...\n",
    "    warning(\"Assuming all cells have the same number of dofs\")\n",
    "\n",
    "    ## Preinit element-wise gradient\n",
    "    grad = DKTCellGradient()\n",
    "\n",
    "    # TODO: check this for parallel operation...\n",
    "    istart, iend = Dh.getOwnershipRange()\n",
    "    #print (\"This process owns the range %d - %d\" % (istart, iend))\n",
    "\n",
    "    for cell_id in range(V.mesh().num_cells()):\n",
    "        cell = Cell(V.mesh(), cell_id)\n",
    "        grad.update(cell)\n",
    "        # Massage dofs into the ordering we require with\n",
    "        # subspace components of each dof consecutive.\n",
    "        dest_rows = np.array(list(chain.from_iterable(zip(dmp1.cell_dofs(cell_id),\n",
    "                                                          dmp2.cell_dofs(cell_id)))),\n",
    "                             dtype=np.intc)\n",
    "        dest_rows = dest_rows[(dest_rows>=istart) & (dest_rows < iend)]\n",
    "        # dofs in V are already in the proper ordering (point eval, gradient eval, ...)\n",
    "        dest_columns = dm.cell_dofs(cell_id)\n",
    "\n",
    "        # NOTE: the copy() is necessary!\n",
    "        #print(\"Setting:\\n\\trows=%s\\n\\tcols=%s\\nwith:\\n%s\" % (dest_rows, dest_columns, M))\n",
    "        Dh.setValues(dest_rows, dest_columns, grad.M.copy())\n",
    "        assert len(dest_rows)*len(dest_columns) ==  grad.M.size, \"duh\"\n",
    "    Dh.assemble()\n",
    "    return Dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It remains only to add an interface to the previous computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_dkt_gradient(V, Vp, u):\n",
    "    \"\"\" Returns the discrete gradient of u.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "        V: Source FunctionSpace of type DKT. (redundant...)\n",
    "        Vp: Target VectorFunctionSpace of type CG2.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        A Function in Vp interpolating the gradient of u.\n",
    "    \"\"\"\n",
    "    Dh = dkt_gradient_operator(V, Vp)\n",
    "    du = Function(Vp)\n",
    "    petsc_u = as_backend_type(u.vector()).vec()\n",
    "    petsc_du = as_backend_type(du.vector()).vec()\n",
    "    \n",
    "    Dh.mult(petsc_u, petsc_du)\n",
    "    du.vector().apply('insert')\n",
    "\n",
    "    return du"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "from utils import ExpressionAD\n",
    "from interpolation import interpolate_hermite\n",
    "import mshr\n",
    "import autograd as ad\n",
    "\n",
    "domain = mshr.Rectangle(Point(1.0, 1.0), Point(2.0, 2.0))\n",
    "mesh = mshr.generate_mesh(domain, 10)\n",
    "W = FunctionSpace(mesh, 'DKT', 3)\n",
    "T = VectorFunctionSpace(mesh, 'Lagrange', 2, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fun(x,y):\n",
    "    return x*y**2 # 2*x**2*y/ad.numpy.sqrt(y)  # This breaks things\n",
    "f = ExpressionAD(fun=fun, degree=3, domain=mesh)\n",
    "w = interpolate_hermite(f, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%timeit  #(~24ms)\n",
    "gf = project(nabla_grad(f), T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%timeit  #(~237ms)\n",
    "dgf = compute_dkt_gradient(W, T, w)\n",
    "_ = plot(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not np.allclose(gf.vector().array(), dgf.vector().array(), atol=1e-11):\n",
    "    dgfa, gfa = dgf.vector().array(), gf.vector().array()\n",
    "    difs = np.abs(dgfa-gfa)/np.maximum(np.abs(dgfa), np.abs(gfa))\n",
    "    print(\"Results differ. Maximum difference = %.3e, mean relative difference = %.3e, std dev = %.3e\" \n",
    "          % (np.max(np.abs(dgfa-gfa)), difs.mean(), difs.std()))\n",
    "    pl.figure(figsize=(20,10))\n",
    "    pl.subplot(1,2,1)\n",
    "    plot(mesh, alpha=0.3)\n",
    "    plot(gf)\n",
    "    pl.title(\"Projection\")\n",
    "    pl.subplot(1,2,2)\n",
    "    plot(mesh, alpha=0.3)\n",
    "    plot(dgf)\n",
    "    _ = pl.title(\"Discrete operator\")\n",
    "else:\n",
    "    print(\"The gradients agree!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how values at the vertices seem to be correct, yet there is still a difference. The issue seems to be mostly at the facet dofs: To see this let us first extract both vertex and facet dofs for each subspace, then compute the list of dofs where the projected gradient and the one computed by the discrete operator disagree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dofs import plot_dofs, plot_field_at_dofs, extract_dofs_with_mask\n",
    "\n",
    "vertex_dofs_0 = extract_dofs_with_mask(T, [0,1,2])   # T.sub(0)\n",
    "vertex_dofs_1 = extract_dofs_with_mask(T, [6,7,8])   # T.sub(1)\n",
    "facet_dofs_0 = extract_dofs_with_mask(T, [3,4,5])    # T.sub(0)\n",
    "facet_dofs_1 = extract_dofs_with_mask(T, [9,10,11])  # T.sub(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bad_dofs(f:Function, g:Function, dofs0, dofs1, plot=True, eps=1e-6):\n",
    "    \"\"\"\n",
    "        f,g: Functions over the same VectorFunctionSpace V of dim=2\n",
    "        dofs0,\n",
    "        dofs1: dofs in V.sub(0), V.sub(1), i.e. they should be of the\n",
    "               same length and go in pairs\"\"\"\n",
    "    assert f.geometric_dimension() == g.geometric_dimension() == 2, \"Need 2d vector valued functions\"\n",
    "    assert f.function_space().element().topological_dimension() == 2, \"Need a 2d domain\"\n",
    "    vf = gf.vector().array()\n",
    "    vg = dgf.vector().array()\n",
    "    assert len(dofs0) == len(dofs1)\n",
    "    num_dofs = len(dofs0)\n",
    "    bad_dofs = []\n",
    "    for i,j in zip(dofs0, dofs1): # range(num_vertices):\n",
    "        try:\n",
    "            dx, dy = vg[i] / vf[i], vg[j] / vf[j]\n",
    "            if not (near(dx, 0, eps=eps) or near(dx, 1, eps=eps)) or\\\n",
    "               not (near(dy, 0, eps=eps), near(dy, 1, eps=eps)):\n",
    "                #print(\"Not colinear with factors = {},{}\".format(dx, dy))\n",
    "                bad_dofs.append(i)\n",
    "        except Exception as e:\n",
    "            print(\"{} Skipping {}\".format(e, i))\n",
    "    print(\"Computed gradients differ at %.2f%% of the dofs.\" % \n",
    "          (100*len(bad_dofs)/num_dofs))\n",
    "    if plot:\n",
    "        plot_dofs(T, bad_dofs, s=60, c='red', alpha=0.6)\n",
    "        plot_field_at_dofs(f, dofs0, dofs1, alpha=0.7, color='orange')\n",
    "        plot_field_at_dofs(g, dofs0, dofs1, alpha=0.8, color='green')\n",
    "    else:\n",
    "        return bad_dofs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are two nice plots. The red dots are placed at dofs where there is a discrepancy between both methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pl.figure(figsize=(24,12))\n",
    "pl.subplot(1,2,1)\n",
    "bad_dofs(gf, dgf, vertex_dofs_0, vertex_dofs_1, eps=1e-3)\n",
    "pl.title(\"Vertex dofs\")\n",
    "pl.subplot(1,2,2)\n",
    "bad_dofs(gf, dgf, facet_dofs_0, facet_dofs_1, eps=1e-3)\n",
    "_ = pl.title(\"Facet dofs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that we are basically ok, up to (sometimes gross) numerical errors (which **may build up!**, so we should be careful here...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating over facets\n",
    "\n",
    "As mentioned above, the construction of the global matrix for $\\nabla_h$ is extremely inefficient for several reasons. One is that we perform useless multiplications with the identity matrix, another that we go twice over all inner facets.\n",
    "\n",
    "FINISH THIS if the C++ implementation doesn't pan out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dofs import facets_dofmap\n",
    "\n",
    "dm = facets_dofmap(T)\n",
    "dm.facet_dofs[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C++ implementation\n",
    "\n",
    "The only sensible way to make the code above faster is to ditch python and integrate the computation of $\\nabla_h$ into the assembly process. A straightforward approach is to basically copy `dolfin/fem/Assembler.*` and adapt it to our problem.\n",
    "\n",
    "But first we need to compute the `DKTGradientOperator` local to a cell. Here follows a first naive implementation (lots of copying etc.):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c++\n",
    "  /// Computes the transformation matrix for $\\nabla_h$ over one cell.    \n",
    "  /// This assumes that the dofs of the \"source\" cell (in DKT) are ordered as\n",
    "  /// \n",
    "  ///      w1, w1_x, w1_y, w2, w2_x, w2_y, w3, w3_x, w3_y,\n",
    "  ///      \n",
    "  /// i.e. point evaluation, partial derivatives for each vertex in turn, and\n",
    "  /// those of the \"target\" cell (in P_2^2) as\n",
    "  /// \n",
    "  ///     t1_0, t1_1, t2_0, t2_1, t3_0, t3_1,\n",
    "  ///     ts1_0, ts1_1, ts2_0, ts2_1, ts3_0, ts3_1\n",
    "  ///     \n",
    "  /// i.e. values at the three vertices, then values at the the midpoints of\n",
    "  /// the opposite sides.\n",
    "  class DKTGradient\n",
    "  {\n",
    "    std::array<double, 3*2> tt;    // tangent vectors\n",
    "    std::array<double, 3*2*2> TT;  // tt_i x tt_i^T (tensor prods.)\n",
    "    std::array<double, 3> ss;      // cell side lengths\n",
    "    std::array<double, 12*9> M;    // gradient matrix\n",
    "    std::vector<double> cc;        // cell coordinates\n",
    "\n",
    "  public:\n",
    "    DKTGradient()\n",
    "    {\n",
    "      /// Initialise the local cell gradient matrix.\n",
    "      /// (interpolates from P_2^2 into DKT)\n",
    "      M.fill(0);  // Necessary?\n",
    "\n",
    "      // Fill identity submatrices\n",
    "      auto RC = [](size_t r, size_t c) -> size_t { return r*9 + c; };\n",
    "      M[RC(0,1)] = 1; M[RC(1,2)] = 1;\n",
    "      M[RC(2,4)] = 1; M[RC(3,5)] = 1;\n",
    "      M[RC(4,7)] = 1; M[RC(5,8)] = 1;\n",
    "    }\n",
    "    \n",
    "    /// Updates the operator matrix for the given Cell\n",
    "    void update(const Cell& cell)\n",
    "    {\n",
    "      // Vector and matrix access helpers for tt, TT and M\n",
    "      // respectively. I guess these will be optimized away...\n",
    "      auto IJ = [](size_t i, size_t j) -> size_t \n",
    "          { return i*2 + j; };\n",
    "      auto IIJ = [](size_t k, size_t i, size_t j) -> size_t \n",
    "          { return 4*k + i*2 + j; };\n",
    "      auto RC = [](size_t r, size_t c) -> size_t \n",
    "          { return r*9 + c; };\n",
    "\n",
    "      cell.get_vertex_coordinates(cc);\n",
    "      \n",
    "      // FIXME: shouldn't this depend on the orientation?\n",
    "      tt[IJ(0,0)] = cc[IJ(2,0)] - cc[IJ(1,0)];\n",
    "      tt[IJ(0,1)] = cc[IJ(2,1)] - cc[IJ(1,1)];      \n",
    "      tt[IJ(1,0)] = cc[IJ(2,0)] - cc[IJ(0,0)];  // Why is this negated?\n",
    "      tt[IJ(1,1)] = cc[IJ(2,1)] - cc[IJ(0,1)];\n",
    "      tt[IJ(2,0)] = cc[IJ(1,0)] - cc[IJ(0,0)];\n",
    "      tt[IJ(2,1)] = cc[IJ(1,1)] - cc[IJ(0,1)];\n",
    "      \n",
    "      auto outer = [&](double x0, double x1) -> std::array<double, 2*2> {\n",
    "        return { x0*x0, x0*x1, x1*x0, x1*x1 };\n",
    "      };\n",
    "      \n",
    "      for (int i=0; i < 3; ++i) {\n",
    "        ss[i] = std::sqrt(tt[IJ(i,0)]*tt[IJ(i,0)] + tt[IJ(i,1)]*tt[IJ(i,1)]);\n",
    "        tt[IJ(i,0)] /= ss[i];\n",
    "        tt[IJ(i,1)] /= ss[i];\n",
    "        auto m = outer(tt[IJ(i,0)], tt[IJ(i,1)]);\n",
    "        TT[IIJ(i,0,0)] = 0.5 - 0.75*m[IJ(0,0)];\n",
    "        TT[IIJ(i,0,1)] =     - 0.75*m[IJ(0,1)];\n",
    "        TT[IIJ(i,1,0)] =     - 0.75*m[IJ(1,0)];\n",
    "        TT[IIJ(i,1,1)] = 0.5 - 0.75*m[IJ(1,1)];\n",
    "        tt[IJ(i,0)] *= -3/(2*ss[i]);\n",
    "      }\n",
    "\n",
    "      // Copy onto the gradient (sub) matrix \n",
    "      // This is quite wasteful...\n",
    "      auto copytt = [&](size_t i, size_t r, size_t c) {\n",
    "        M[RC(r,c)] = tt[IJ(i,0)]; M[RC(r+1,c)] = tt[IJ(i,1)];\n",
    "      };\n",
    "      auto copy_tt = [&](size_t i, size_t r, size_t c) {\n",
    "        M[RC(r,c)] = -tt[IJ(i,0)]; M[RC(r+1,c)] = -tt[IJ(i,1)];\n",
    "      };\n",
    "      auto copyTT = [&](size_t i, size_t r, size_t c) {\n",
    "        M[RC(r,c)] = TT[IIJ(i,0,0)]; M[RC(r,c+1)] = TT[IIJ(i,0,1)];\n",
    "        M[RC(r+1,c)] = TT[IIJ(i,1,0)]; M[RC(r+1,c+1)] = TT[IIJ(i,1,1)];\n",
    "      };\n",
    "\n",
    "      copytt(0, 3, 2);\n",
    "      copyTT(0, 3, 3);\n",
    "      copy_tt(0, 3, 4);\n",
    "      copyTT(0, 3, 5);\n",
    "      \n",
    "      copytt(0, 4, 0);\n",
    "      copyTT(0, 4, 1);\n",
    "      copy_tt(0, 4, 4);\n",
    "      copyTT(0, 4, 5);\n",
    "      \n",
    "      copytt(0, 5, 0);\n",
    "      copyTT(0, 5, 1);\n",
    "      copy_tt(0, 5, 2);\n",
    "      copyTT(0, 5, 3);\n",
    "    }\n",
    "\n",
    "    /// Transform local dof coefficients from P_3^red to P_2^2\n",
    "    void apply(const std::vector<double>& p3coeffs, \n",
    "               std::vector<double>& p22coeffs)\n",
    "    {\n",
    "      // Copy coefficients for partial derivatives\n",
    "      p22coeffs[0] = p3coeffs[1];  p22coeffs[1] = p3coeffs[2];\n",
    "      p22coeffs[2] = p3coeffs[4];  p22coeffs[3] = p3coeffs[5];\n",
    "      p22coeffs[4] = p3coeffs[7];  p22coeffs[5] = p3coeffs[8];\n",
    "\n",
    "      // Matrix * vec multiplication\n",
    "      for (auto i = 0; i < 6; ++i) {\n",
    "        p22coeffs[6+i] = 0;\n",
    "        for (auto j = 0; j < 9; ++j) {\n",
    "           p22coeffs[6+i] += M[i*9+j]*p3coeffs[j];\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once that works we use it in the assembler. We need to be careful with:\n",
    "\n",
    "1. Combining the dofmaps from two forms: this won't be that hard. We compute the local tensor for the P2 form then compute $M^T A M$ and apply the local-to-global dofmap for the DKT space in order to populate the global tensor.\n",
    "\n",
    "2. Precomputing the sparsity pattern. This is done in `AssemblerBase::init_global_tensor(GenericTensor& A, const Form& a)` and relies on the FunctionSpaces associated to the Form being assembled, so we'd need to rewrite this.\n",
    "\n",
    "The resulting assembler is available in `KirchhoffAssembler.cpp`, which is basically a copy of `dolfin/fem/Assembler.cpp` with modifications to call the routines in `DKTGradient` at the relevant places."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "Ideally we'd like to make the assembler available from the python interface. However this involves a bit of boilerplate, so I'll leave the following stub here in case I decide to come back to it in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dolfin import *\n",
    "import numpy\n",
    "import os\n",
    "\n",
    "src_dir = \"/home/fenics/local/src/nonlinear-kirchhoff/\"\n",
    "typemap =\"\"\"\n",
    "%typemap(in) (const std::vector<double>& cc) {\n",
    "    if (! PyArray_Check($input)) {\n",
    "        PyErr_SetString(PyExc_TypeError, \"Not a NumPy array\");\n",
    "        return NULL;\n",
    "    }\n",
    "    PyArrayObject* pyarray;\n",
    "    pyarray = (PyArrayObject*)$input;\n",
    "    if (!(PyArray_TYPE(pyarray) == NPY_DOUBLE)) {\n",
    "      PyErr_SetString(PyExc_TypeError, \"Not a NumPy array of doubles\");\n",
    "      return NULL;\n",
    "    }\n",
    "    auto vec = std::vector<double>(pyarray->dimensions[0]);\n",
    "    for(int i = 0; i < pyarray->dimensions[0]; ++i)\n",
    "        vec[i] = pyarray->data[i];\n",
    "    $1 = vec;\n",
    "}\n",
    "\"\"\"\n",
    "with open(src_dir + \"DKTGradient.h\", \"r\") as header_file:\n",
    "    code = header_file.read()\n",
    "    dkg = compile_extension_module(code=code,\n",
    "                                   additional_declarations=typemap,\n",
    "                                   source_directory=src_dir,\n",
    "                                   sources=[\"DKTGradient.cpp\"], \n",
    "                                   include_dirs=[\".\", os.path.abspath(src_dir), \"/usr/local/slepc-64/include/\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = VectorFunctionSpace(UnitSquareMesh(10,10, \"right\"), \"DKT\", degree=3, dim=3)\n",
    "c = Cell(W.mesh(), 0)\n",
    "\n",
    "dg = dkg.DKTGradient\n",
    "dg.update(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump\n",
    "\n",
    "Everything and anything..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "from dofs import plot_dofs\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2d = vertex_to_dof_map(W3)\n",
    "print(v2d)\n",
    "for vit in range(msh.num_vertices()):\n",
    "    v = Vertex(msh, vit)\n",
    "    for sub in range(3):\n",
    "        print(v2d[9*v.index()+3*sub+1])\n",
    "plot_dofs(W3, [15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msh = UnitSquareMesh(1,1)\n",
    "W3 = VectorFunctionSpace(msh, \"DKT\", 3, dim=3)\n",
    "T6 = TensorFunctionSpace(msh, \"Lagrange\", 2, shape=(3,2))\n",
    "dmW = W3.sub(0).dofmap()\n",
    "c = Cell(msh, 0)\n",
    "print(c.get_coordinate_dofs())\n",
    "dofs = dmW.cell_dofs(0)\n",
    "print(dofs)\n",
    "# local dofs for the P2 element are eval at v0,v1,v2, then eval at s1,s2,s3:\n",
    "#plot_dofs(W3, dofs[dmW.tabulate_entity_dofs(0,2)])\n",
    "#plot_dofs(W3, [6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmT = T6.sub(1).dofmap()\n",
    "dmT.cell_dofs(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dolfin import *\n",
    "import nbimporter\n",
    "from discrete_gradient import DKTCellGradient\n",
    "import numpy as np\n",
    "\n",
    "#ACHTUNG!\n",
    "np.set_printoptions(precision=4, linewidth=120)\n",
    "\n",
    "def cppprint(x, prec=4):\n",
    "    m, n = x.shape\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            print(str(np.round(x[i,j], prec)) + \", \", end='')\n",
    "        print(\"\")\n",
    "\n",
    "domain = UnitSquareMesh(1,1)\n",
    "V = VectorFunctionSpace(domain, \"Lagrange\", dim=2, degree=2)\n",
    "u = TrialFunction(V)\n",
    "v = TestFunction(V)\n",
    "\n",
    "a = inner(nabla_grad(u), nabla_grad(v))*dx\n",
    "A = assemble(a)\n",
    "\n",
    "grad = DKTCellGradient()\n",
    "Aa = A.array()\n",
    "A2 = np.zeros((12,12))\n",
    "dm = V.dofmap()\n",
    "for cell_id in range(V.mesh().num_cells()):\n",
    "    cell = Cell(V.mesh(), cell_id)\n",
    "    print(cell.get_coordinate_dofs())\n",
    "    grad.update(cell)\n",
    "    print(\"***** M ******\")\n",
    "    cppprint(grad.M)\n",
    "    print(\"***** A2 ******\")\n",
    "    l2g = dm.cell_dofs(cell_id)   # local to global mapping for cell \n",
    "    for i,j in np.ndindex(A2.shape):\n",
    "        A2[i,j] = Aa[l2g[i], l2g[j]]\n",
    "    cppprint(A2)\n",
    "    print(\"============= Mt @ A2 @ M ================\")\n",
    "    cppprint(grad.M.transpose() @ A2 @ grad.M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick retesting of the assembly of the matrix for $\\nabla_h$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mesh = UnitSquareMesh(1,1)\n",
    "W = FunctionSpace(mesh, 'DKT', 3)\n",
    "T = VectorFunctionSpace(mesh, 'Lagrange', 2, dim=2)\n",
    "\n",
    "w = Function(W)\n",
    "dmw = W.dofmap()\n",
    "wdofs_cell0 = dmw.cell_dofs(0)\n",
    "wdofs_cell1 = dmw.cell_dofs(1)\n",
    "new_vals = np.zeros_like(w.vector().array())\n",
    "new_vals[wdofs_cell0] = [2,1,1,2,1,1,2,1,1]\n",
    "w.vector().set_local(new_vals)\n",
    "\n",
    "\n",
    "Dh = DKTCellGradient(T.element().space_dimension(), W.element().space_dimension())\n",
    "Dh.update(Cell(W.mesh(), 0))\n",
    "product = Dh.M@w.vector().array()[wdofs_cell0]\n",
    "\n",
    "dw2 = Function(T)\n",
    "dm0 = T.sub(0).dofmap()\n",
    "dm1 = T.sub(1).dofmap()\n",
    "tdofs0_cell0 = dm0.cell_dofs(0)\n",
    "tdofs1_cell0 = dm1.cell_dofs(0)\n",
    "#### <strike>DON'T FIXME!!!! this is NOT WRONG:</strike>\n",
    "dest_rows = np.array(list(chain.from_iterable(zip(tdofs0_cell0,tdofs1_cell0))), dtype=np.intc)\n",
    "print(tdofs0_cell0, tdofs1_cell0)\n",
    "print(dest_rows)\n",
    "#### found manually:\n",
    "dest_rows = [2, 3, 12, 13, 4, 5, 14, 15, 6, 7, 16, 17]\n",
    "new_vals2 = np.zeros_like(dw2.vector().array())\n",
    "new_vals2[dest_rows] = product\n",
    "dw2.vector().set_local(new_vals2)\n",
    "print(product)\n",
    "print(Dh.M)\n",
    "\n",
    "pl.figure(figsize=(12,6))\n",
    "plot(T.mesh(), alpha=0.3)\n",
    "vertex_dofs_0 = extract_dofs_with_mask(T, [0,1,2])   # T.sub(0)\n",
    "vertex_dofs_1 = extract_dofs_with_mask(T, [6,7,8])   # T.sub(1)\n",
    "facet_dofs_0 = extract_dofs_with_mask(T, [3,4,5])    # T.sub(0)\n",
    "facet_dofs_1 = extract_dofs_with_mask(T, [9,10,11])  # T.sub(1)\n",
    "plot_field_at_dofs(dw2, vertex_dofs_0, vertex_dofs_1, color='orange')\n",
    "plot_field_at_dofs(dw2, facet_dofs_0, facet_dofs_1, color='green')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {
    "height": "167px",
    "width": "254px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {
    "height": "1247px",
    "left": "0px",
    "right": "1601px",
    "top": "84px",
    "width": "368px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
