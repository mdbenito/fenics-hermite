{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Discrete-gradient-operators\" data-toc-modified-id=\"Discrete-gradient-operators-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Discrete gradient operators</a></div><div class=\"lev1 toc-item\"><a href=\"#Gradient-of-a-piecewise-linear-function\" data-toc-modified-id=\"Gradient-of-a-piecewise-linear-function-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Gradient of a piecewise linear function</a></div><div class=\"lev2 toc-item\"><a href=\"#A-detour-using-projections\" data-toc-modified-id=\"A-detour-using-projections-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>A detour using projections</a></div><div class=\"lev3 toc-item\"><a href=\"#How-does-the-assembly-of-the-projection-problem-work?\" data-toc-modified-id=\"How-does-the-assembly-of-the-projection-problem-work?-211\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>How does the assembly of the projection problem work?</a></div><div class=\"lev2 toc-item\"><a href=\"#And-back\" data-toc-modified-id=\"And-back-22\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>And back</a></div><div class=\"lev2 toc-item\"><a href=\"#Using-the-PETSc-backend\" data-toc-modified-id=\"Using-the-PETSc-backend-23\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Using the PETSc backend</a></div><div class=\"lev1 toc-item\"><a href=\"#Discrete-gradient-for-DKT\" data-toc-modified-id=\"Discrete-gradient-for-DKT-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Discrete gradient for DKT</a></div><div class=\"lev2 toc-item\"><a href=\"#Tests\" data-toc-modified-id=\"Tests-31\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Tests</a></div><div class=\"lev2 toc-item\"><a href=\"#Iterating-over-facets\" data-toc-modified-id=\"Iterating-over-facets-32\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Iterating over facets</a></div><div class=\"lev2 toc-item\"><a href=\"#C++-implementation\" data-toc-modified-id=\"C++-implementation-33\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>C++ implementation</a></div><div class=\"lev2 toc-item\"><a href=\"#Tests\" data-toc-modified-id=\"Tests-34\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Tests</a></div><div class=\"lev1 toc-item\"><a href=\"#Dump\" data-toc-modified-id=\"Dump-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Dump</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete gradient operators\n",
    "\n",
    "This notebook collects my progress towards the definition of the operator\n",
    "\n",
    "$$\\nabla_h:W_h \\rightarrow \\Theta_h$$\n",
    "\n",
    "as defined in (Bartels, 2015) ยง8.2.1. We proceed as follows:\n",
    "\n",
    "1. Computation of (global) exact gradient for CG1 functions, i.e. as an operator from CG1 into CG0. This serves to test the idea with a simpler operator. See [Gradient of a piecewise linear function](#Gradient-of-a-piecewise-linear-function).\n",
    "2. Computation of (global) gradient $\\nabla_h$ for DKT functions, i.e. as an operator from DKT into CG2^2. See [Discrete gradient for DKT](#Discrete-gradient-for-DKT). This implementation basically works, but being pure python is extremely slow. A small improvement might be achieved by [Iterating over facets](#Iterating-over-facets). However we are building a huge matrix and multiplying the system matrix by it and its transpose, which is very slow.\n",
    "3. Implementation in C++ of the assembly process. This requires a new assembler for each problem or at least for each family of related ones, but should be pretty straightfoward if one uses FEniCS compiled forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## The usual boilerplate...\n",
    "\n",
    "from itertools import chain\n",
    "from dolfin import *\n",
    "from petsc4py import PETSc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "%matplotlib inline\n",
    "\n",
    "np.set_printoptions(precision=3, linewidth=150)\n",
    "\n",
    "def __nbinit__():\n",
    "    global __all__\n",
    "    __all__ = ['compute_discrete_gradient_dense', \n",
    "               'compute_discrete_gradient',\n",
    "               'compute_dkt_gradient']\n",
    "    global parameters\n",
    "    # instruct FFC to produce the code for evaluate_basis_derivatives()\n",
    "    parameters[\"form_compiler\"][\"no-evaluate_basis_derivatives\"] = False\n",
    "\n",
    "__nbinit__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient of a piecewise linear function\n",
    "\n",
    "Let $u \\in V$, where $V$ is a CG1 finite element space. Then $u'$ is piecewise constant (i.e. it is in DG0). We want to compute the matrix $D_h$ for \n",
    "\n",
    "$$\\nabla_h:V \\rightarrow V'$$\n",
    "\n",
    "such that the $j$-th coefficient of $u'$ in $V'$, $(\\nabla u)_j = (\\nabla_h u)_j =: u_j'$. Note that we are using the same  notation for a function $u$ and for the vector of its coefficients in the basis of the space it lives on.\n",
    "\n",
    "Since $u'$ is not in the same space as $u$, in FEniCS we need to `project()` it onto $V$, which is costly. What we are doing is to manually compute the derivative. This should be easy since\n",
    "\n",
    "$$ u = \\sum_i u_i \\phi_i $$\n",
    "\n",
    "where $V = \\text{span} \\{\\phi_i\\} $, and therefore, by linearity:\n",
    "\n",
    "$$ u' = \\sum_i u_i \\phi_i'. $$\n",
    "\n",
    "We could use `evaluate_basis_derivatives()` to evaluate the $\\phi_i'$ and then we would be able to evaluate $u'$. However, we are interested in obtaining new coefficients $u_i'$ such that\n",
    "\n",
    "$$ u' = \\sum_i u_i \\psi_i', $$\n",
    "\n",
    "where\n",
    "\n",
    "assembling the transformation matrix by hand.\n",
    "\n",
    "We define $V' = \\text{span} \\{\\phi_i'\\}$, a DG0 space, and compute the linear mapping of dofs from $V$ to $V'$, then apply this mapping to the coefficients of $u$ and construct $u' \\in V'$ with these coefficients.\n",
    "\n",
    "For simplicity, consider first a 1D mesh. Let $i \\in \\{0,1,...,M\\}$ be an index over the cells of the mesh such that:\n",
    "\n",
    "* $u_i,u_{i+1}$ are the coefficients for the dofs in $V$ over cell $i$\n",
    "* $u_i'$ is the coefficient for the only dof in $V'$ (the constant 1) over cell $i$\n",
    "\n",
    "Then we only need to compute $\\phi_i'$ for each $i$ to obtain the new vector of coefficients $(u_i')_{i=0}^{N_{V'}}$:\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{ccccc}\n",
    "  \\phi_0' & \\phi_1' &  &  & \\\\\n",
    "  & \\phi_1' & \\phi_2' &  & \\\\\n",
    "  &  & \\phi_2' & \\phi_3' & \\\\\n",
    "  &  &  & \\phi_3' & \\phi_4'\n",
    "\\end{array}\\right)  \\left(\\begin{array}{c}\n",
    "  u_0\\\\\n",
    "  u_1\\\\\n",
    "  u_2\\\\\n",
    "  u_3\\\\\n",
    "  u_4\n",
    "\\end{array}\\right) = \\left(\\begin{array}{c}\n",
    "  u_0'\\\\\n",
    "  u_1'\\\\\n",
    "  u_2'\\\\\n",
    "  u_3'\n",
    "\\end{array}\\right).$$\n",
    "\n",
    "Note that in this particular instance and with the assumptions made, we obtain a band matrix and computing $u'$ is actually a matter of computing the dot product of a \"subset\" of the vector of constants $(\\phi_0', ..., \\phi_{N_{V'}}')$ with $u$.\n",
    "\n",
    "However in other situations, e.g. in higher dimensions, the ordering of the nodes won't be as convenient. We proceed then more generally as follows: Let $i$ run over all cell indices and let $\\iota_i$ be the local-to-global mappings for $V$ and $\\iota_i'$ for $V'$. Initialise the matrix $M$ of the discrete gradient to zero. At each step of the construction of $M$ we edit row $r = \\iota_i'(0)$, i.e. the row whose product by $(u)_j$ will return the coefficient for the dof in $V'$ corresponding to cell $i$.\n",
    "\n",
    "Now, for every vertex $v_j,\\ j \\in \\{0,...,d\\}$ in cell $i$ compute the value of the derivative of the global dof $\\phi_{\\iota_r(j)}$ and set\n",
    "\n",
    "$$M_{r j} = \\phi_{\\iota_r(j)},\\ j \\in \\{0,...,d\\}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discrete_gradient_operator_dense(V, Vp):\n",
    "    \"\"\" Build dense matrix of the discrete gradient\n",
    "    operator between V and Vp.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "        V: Source FunctionSpace of type CG1.\n",
    "        Vp: Target (Vector)FunctionSpace of type DG0.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        Dh: np.ndarray of shape (Vp.dim(), V.dim()) such that,\n",
    "            for u in V:\n",
    "        \n",
    "                np.dot(Dh, u.vector().array())\n",
    "              \n",
    "            yields the vector of coefficients of the discrete\n",
    "            gradient of u in Vp.\n",
    "    \"\"\"\n",
    "    gdim = V.mesh().geometry().dim()\n",
    "    assert V.ufl_element().shortstr()[:3] == 'CG1', \"Need a CG1 source space\"\n",
    "    assert Vp.ufl_element().shortstr()[:14] == 'Vector<%d x DG0' % gdim or\\\n",
    "           Vp.ufl_element().shortstr()[:3] == 'DG0',\\\n",
    "           \"I need a DG0 (Vector)FunctionSpace of the right number of components\"\n",
    "    assert Vp.mesh().num_cells() * gdim == Vp.dim(),\\\n",
    "           \"FunctionSpace dimensions don't match.\"\n",
    "    assert Vp.mesh().num_cells() == V.mesh().num_cells(),\\\n",
    "           \"Meshes don't match\"\n",
    "\n",
    "    dm = V.dofmap()\n",
    "    dmp = Vp.dofmap()\n",
    "    e = V.element()\n",
    "    coords = V.tabulate_dof_coordinates().reshape((-1, gdim))\n",
    "    coordsp = Vp.tabulate_dof_coordinates().reshape((-1, gdim))\n",
    "\n",
    "    Dh = np.zeros((Vp.dim(), V.dim()))\n",
    "    warning(\"Assuming all cells have the same number of dofs\")\n",
    "    vals = np.zeros(dm.num_element_dofs(0)*gdim)\n",
    "    for cell_id in range(Vp.mesh().num_cells()):\n",
    "        rows = dmp.cell_dofs(cell_id)\n",
    "        columns = dm.cell_dofs(cell_id)\n",
    "        dof_coords = coords[columns,:]\n",
    "        point = dof_coords[0] # Any point in the (closure of the) cell will do.\n",
    "        e.evaluate_basis_derivatives_all(1, vals, point, dof_coords, 1)\n",
    "        #print(\"rows=%s, columns=%s, point=%s, dof_coords=%s, vals=%s\" %\n",
    "        #     (rows, columns, point, dof_coords, vals.round(2)))\n",
    "        # Reshape [df_1/dx, df_1/dy, df_2/dx, df_2/dy, ...] into\n",
    "        # [[df_1/dx, df_2/dx, ...],[df_1/dy, df_2/dy, ...]] :\n",
    "        #Dh[FIXTHIS:\"rows, columns\"] = vals.reshape((-1, gdim)).T.copy()\n",
    "        for offset in range(gdim):\n",
    "            Dh[rows[offset], columns] = vals[offset::gdim].copy()\n",
    "\n",
    "    return Dh\n",
    "\n",
    "def compute_discrete_gradient_dense(V, Vp, u):\n",
    "    \"\"\" Returns the discrete gradient of u.\n",
    "    NOTE: this implementation uses dense matrices internally.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "        V: Source FunctionSpace of type CG1. (redundant...)\n",
    "        Vp: Target (Vector)FunctionSpace of type DG0.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        A Function in Vp interpolating the gradient of u.\n",
    "    \"\"\"\n",
    "\n",
    "    Dh = discrete_gradient_operator_dense(V, Vp)\n",
    "    up = Function(Vp)\n",
    "    up.vector().set_local(np.dot(Dh, u.vector().array()))\n",
    "    up.vector().apply('insert')\n",
    "\n",
    "    return up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test the previous code we start with an expression whose derivative we can compute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEFT, RIGHT = -pi, 2*pi\n",
    "mesh = IntervalMesh(100, LEFT, RIGHT)\n",
    "V = FunctionSpace(mesh, \"CG\", 1)\n",
    "Vp = FunctionSpace(mesh, \"DG\", 0)\n",
    "\n",
    "#x = SpatialCoordinate(mesh)\n",
    "#f = sin(x[0])\n",
    "#u = project(f, V)\n",
    "u = interpolate(Expression(\"sin(x[0])\", element=V.ufl_element()), V)\n",
    "dg_u = compute_discrete_gradient_dense(V, Vp, u)\n",
    "\n",
    "# Test\n",
    "#up = project(f.dx(0), Vp)\n",
    "up = interpolate(Expression(\"cos(x[0])\", element=Vp.ufl_element()), Vp)\n",
    "print(\"Error: %e\" % (dg_u.vector() - up.vector()).norm('linf'))\n",
    "plot(up)\n",
    "plot(dg_u)\n",
    "_ = pl.xlim((LEFT*1.1, RIGHT*1.1)), pl.ylim((-1.1,1.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A detour using projections\n",
    "\n",
    "The following method computes the solution via a projection onto DG0 but instead of letting FEniCS assemble the mass matrix, we make use of the fact that it is diagonal with entries given by the cell volume. Then it is trivial to invert it and multiply the rhs of the projection problem. The idea and code are taken from [this question on FEniCS Q&A](https://fenicsproject.org/qa/12634/gradient-of-a-cg-order-one-element?show=12646#a12646)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mesh = UnitCubeMesh(10, 10, 10)\n",
    "W = FunctionSpace(mesh, 'CG', 1)\n",
    "Q = VectorFunctionSpace(mesh, 'DG', 0)\n",
    "p, q = TrialFunction(Q), TestFunction(Q)\n",
    "\n",
    "f = Function(W)\n",
    "f_vec = f.vector()\n",
    "f_vec.set_local(np.random.rand(f_vec.local_size()))\n",
    "f_vec.apply('insert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first compute the gradient via a projection. **NOTE** that the computation for the form $L$ will have to interpolate $f$, which is in \"W\", into the space $T$ where the test function $q$ lives in order to compute the integral. [WHY?]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "L = inner(nabla_grad(f), q)*dx\n",
    "M = assemble(inner(p, q)*dx)\n",
    "b = assemble(L)\n",
    "\n",
    "grad_f0 = Function(Q)\n",
    "x0 = grad_f0.vector()\n",
    "_ = solve(M, x0, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MiroK's idea (which amounts to building our matrix $D_h$) in the forum is to \"assemble the action of Minv onto the rhs - no mass matrix needed\". The improvement in speed is huge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "MinvL = (1/CellVolume(Q.mesh()))*inner(nabla_grad(f), q)*dx\n",
    "x = assemble(MinvL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grad_f = Function(Q, x)\n",
    "print(\"Error: %e\" % (x - x0).norm('linf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the first approach we compute $M$, then solve $M x = b$ and in the second we compute $M^{-1}$ directly to obtain $x = M^{-1} b$.\n",
    "\n",
    "### How does the assembly of the projection problem work?\n",
    "\n",
    "Write the compiled form to a file and check `dgrad_cell_integral_0_otherwise::tabulate_tensor()` for the implementation. Recall that `u.dx()` in the form is the derivative of a terminal node in UFL so it is the responsibility of FFC to fill that. How does this work again?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ffc\n",
    "with open(\"/tmp/dgrad.h\", \"wt\") as fd:\n",
    "    out = ffc.compile_form(MinvL, prefix=\"dgrad\")\n",
    "    fd.write(out[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And back\n",
    "\n",
    "We can use our function instead. This will be much slower and also use a lot of memory because of the dense matrix we are using. The error is different, though. **Could this be significant?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dg_f = compute_discrete_gradient_dense(W, Q, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Error: %e\" % (dg_f.vector() - grad_f0.vector()).norm('linf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the PETSc backend\n",
    "\n",
    "By using PETSc We can achieve a (back-of-the-napkin) speed improvement of up to 50 times wrt. the implementation with dense matrices and stay in the same range of running times than `project()`. We can only test for manageable matrix sizes but we can now of course handle bigger matrices. The speedup depends greatly on the size of the matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discrete_gradient_operator(V, Vp):\n",
    "    \"\"\" Build sparse PETSc matrix of the discrete gradient\n",
    "    operator between V and Vp.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "        V: Source FunctionSpace of type CG1.\n",
    "        Vp: Target (Vector)FunctionSpace of type DG0.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        Dh: PETSc sparse matrix such that for u in V,\n",
    "              Dh * u.vector().array()\n",
    "            yields the vector of coefficients of the discrete\n",
    "            gradient of u in Vp.\n",
    "    \"\"\"   \n",
    "    gdim = V.mesh().geometry().dim()\n",
    "    assert V.ufl_element().shortstr()[:3] == 'CG1',\\\n",
    "           \"Need a CG1 source space\"\n",
    "    assert Vp.ufl_element().shortstr()[:14] == 'Vector<%d x DG0' % gdim or\\\n",
    "           Vp.ufl_element().shortstr()[:3] == 'DG0',\\\n",
    "           \"I need a DG0 (Vector)FunctionSpace of the right number of components\"\n",
    "    assert Vp.mesh().num_cells() * gdim == Vp.dim(),\\\n",
    "           \"FunctionSpace dimensions don't match\"\n",
    "    assert Vp.mesh().num_cells() == V.mesh().num_cells(),\\\n",
    "           \"Number of cells in the meshes don't match\"\n",
    "\n",
    "    dm = V.dofmap()\n",
    "    dmp = Vp.dofmap()\n",
    "    e = V.element()\n",
    "    coords = V.tabulate_dof_coordinates().reshape((-1, gdim))\n",
    "    coordsp = Vp.tabulate_dof_coordinates().reshape((-1, gdim))\n",
    "\n",
    "    Dh = PETSc.Mat()\n",
    "    Dh.create(PETSc.COMM_WORLD)  # What is this?\n",
    "    Dh.setSizes([Vp.dim(), V.dim()])\n",
    "    Dh.setType(\"aij\")\n",
    "    Dh.setUp()\n",
    "    warning(\"Assuming all cells have the same number of dofs\")\n",
    "    vals = np.zeros(dm.num_element_dofs(0)*gdim)\n",
    "    # TODO: check this for parallel operation...\n",
    "    istart, iend = Dh.getOwnershipRange()\n",
    "    print (\"This process owns the range %d - %d\" % (istart, iend))\n",
    "    for cell_id in range(Vp.mesh().num_cells()):\n",
    "        rows = dmp.cell_dofs(cell_id)\n",
    "        rows = rows[(rows>=istart) & (rows < iend)]\n",
    "        columns = dm.cell_dofs(cell_id)\n",
    "        dof_coords = coords[columns,:]\n",
    "        point = dof_coords[0] # Any point in the (closure of the) cell will do.\n",
    "        e.evaluate_basis_derivatives_all(1, vals, point, dof_coords, 1)\n",
    "        #print(\"rows=%s, columns=%s, point=%s, dof_coords=%s, vals=%s\" %\n",
    "        #     (rows, columns, point, dof_coords, vals.round(2)))\n",
    "        # Reshape [df_1/dx, df_1/dy, df_2/dx, df_2/dy, ...] into\n",
    "        # [[df_1/dx, df_2/dx, ...],[df_1/dy, df_2/dy, ...]] :\n",
    "        # NOTE: the copy() is necessary!\n",
    "        Dh.setValues(rows, columns, vals.reshape((-1, gdim)).T.copy())\n",
    "    Dh.assemble()\n",
    "    return Dh\n",
    "\n",
    "def compute_discrete_gradient(V, Vp, u):\n",
    "    \"\"\" Returns the discrete gradient of u.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "        V: Source FunctionSpace of type CG1. (redundant...)\n",
    "        Vp: Target (Vector)FunctionSpace of type DG0.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        A Function in Vp interpolating the gradient of u.\n",
    "    \"\"\"\n",
    "    Dh = discrete_gradient_operator(V, Vp)\n",
    "    du = Function(Vp)\n",
    "    petsc_u = as_backend_type(u.vector()).vec()\n",
    "    petsc_du = as_backend_type(du.vector()).vec()\n",
    "    \n",
    "    Dh.mult(petsc_u, petsc_du)\n",
    "    du.vector().apply('insert')\n",
    "\n",
    "    return du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEFT, RIGHT = -pi, 2*pi\n",
    "mesh = IntervalMesh(1000, LEFT, RIGHT)\n",
    "V = FunctionSpace(mesh, \"CG\", 1)\n",
    "Vp = FunctionSpace(mesh, \"DG\", 0)\n",
    "\n",
    "#Dh = discrete_gradient_operator(V, Vp)\n",
    "u = interpolate(Expression(\"sin(x[0])\", degree=1), V)\n",
    "du0 = interpolate(Expression(\"cos(x[0])\", degree=0), Vp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "du = compute_discrete_gradient(V, Vp, u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Error: %e\" % (du.vector() - du0.vector()).norm('linf'))\n",
    "_ = plot(du)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is faster than the solution with dense matrices but obviously still much slower (a little below 40 times) than building $M^{-1}$ directly because of the hideously slow python loops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dgrad_f = compute_discrete_gradient(W, Q, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Error: %e\" % (dgrad_f.vector() - grad_f0.vector()).norm('linf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete gradient for DKT\n",
    "\n",
    "We proceed with the implementation of\n",
    "\n",
    "$$\\nabla_h:W_h \\rightarrow \\Theta_h$$\n",
    "\n",
    "for Functions over the whole space. Recall that the operator $\\nabla_h$ is given cell-wise by the matrix\n",
    "\n",
    "\\begin{equation}\n",
    "  M_T = \\left(\\begin{array}{cccccc}\n",
    "    0 & I_2 & 0 & 0 & 0 & 0\\\\\n",
    "    0 & 0 & 0 & I_2 & 0 & 0\\\\\n",
    "    0 & 0 & 0 & 0 & 0 & I_2\\\\\n",
    "    0 & 0 & \\tilde{t}_{S_1} & \\tilde{T}_{S_1} & - \\tilde{t}_{S_1} &\n",
    "    \\tilde{T}_{S_1}\\\\\n",
    "    \\tilde{t}_{S_2} & \\tilde{T}_{S_2} & 0 & 0 & - \\tilde{t}_{S_2} &\n",
    "    \\tilde{T}_{S_2}\\\\\n",
    "    \\tilde{t}_{S_3} & \\tilde{T}_{S_3} & - \\tilde{t}_{S_3} & \\tilde{T}_{S_3} &\n",
    "    0 & 0\n",
    "  \\end{array}\\right) \\in \\mathbb{R}^{12 \\times 9}, \\quad \\tilde{T}_{S_l} =\n",
    "  \\tfrac{- 3}{4} t_{S_l} t_{S_l}^{\\top} + \\tfrac{1}{2} I_2, \n",
    "  \\quad \\tilde{t}_{S_l} = \\tfrac{- 3}{2\n",
    "  | S_l |} t_{S_l}\n",
    "\\end{equation}\n",
    "\n",
    "Since most of $M_T$ won't change, we compute it in two steps: an initialization step, which sets the fixed entries and a per-cell step, where we set the entries for each cell.\n",
    "\n",
    "**NOTE:** Because we are not using a hierarchical basis, the computations in (Bartels, 2015) Ch.8 don't apply verbatim and our $\\tilde{T}_{S_l}$ differs by a constant $+ \\tfrac{1}{2} I_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DKTCellGradient(object):\n",
    "    \"\"\" Computes the transformation matrix for $\\nabla_h$ over one cell.\n",
    "    \n",
    "    This assumes that the dofs of the \"source\" cell (in DKT) are ordered as\n",
    "    \n",
    "         w1, w1_x, w1_y, w2, w2_x, w2_y, w3, w3_x, w3_y,\n",
    "         \n",
    "    i.e. point evaluation, partial derivatives for each vertex in turn, and\n",
    "    those of the \"target\" cell (in P_2^2) as\n",
    "    \n",
    "        t1_0, t1_1, t2_0, t2_1, t3_0, t3_1,\n",
    "        ts1_0, ts1_1, ts2_0, ts2_1, ts3_0, ts3_1\n",
    "        \n",
    "    i.e. values at the three vertices, then values at the the midpoints of\n",
    "    the opposite sides.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\" Initialize the local cell gradient matrix.\n",
    "        The matrix is 12x9  (target's local space_dimension() x source's \n",
    "        local space_dimension().\"\"\"\n",
    "\n",
    "        self.M = np.zeros((12, 9), dtype=np.float)\n",
    "        rows = [[0,1], [2,3], [4,5], [6,7], [8,9], [10,11]]\n",
    "        # Hack to make fancy indexing with lists work:\n",
    "        # if one of rows[i], self.cols[j] has shape 2,1, broadcasting will actually\n",
    "        # compute the outer product of rows[i] and cols[j], instead of a \"zip\"\n",
    "        # of sorts.\n",
    "        self.rows = list(map(lambda x: np.array(x).reshape(2,1), rows))\n",
    "        self.cols = [0, [1,2], 3, [4,5], 6, [7,8]]\n",
    "        #self.cols = list(map(lambda x: np.array(x).reshape((-1,1)), self.cols))\n",
    "        # TODO: Note that storing these copies of the identity is silly\n",
    "        # When we apply the operator, it would make more sense to just copy\n",
    "        # the partial derivative dofs (which is what these first 3 multiplications \n",
    "        # with the identity do), then multiply with the lower part of M...\n",
    "        # (But we still need these here if we insist on building a global matrix\n",
    "        # which acts on all dofs of the space simultaneously)\n",
    "        self.M[self.rows[0], self.cols[1]] = np.eye(2)\n",
    "        self.M[self.rows[1], self.cols[3]] = np.eye(2)\n",
    "        self.M[self.rows[2], self.cols[5]] = np.eye(2)\n",
    "\n",
    "        # We are only defined for triangles (to do: assert this)\n",
    "        # tdim = 2, num_vertices = 3\n",
    "\n",
    "        # Init temporaries\n",
    "        self.tt = np.empty((3, 2))\n",
    "        self.TT = np.empty((3, 2, 2))\n",
    "        self.ss = np.empty(3)\n",
    "\n",
    "    def update(self, cell:Cell):\n",
    "        \"\"\"Update the mutating entries of M for the given cell.\"\"\"\n",
    "        cc = cell.get_vertex_coordinates().reshape(-1, 2)\n",
    "        self.tt[0] = (cc[2] - cc[1])\n",
    "        self.tt[1] = -(cc[0] - cc[2])  # Magic: why was this minus sign here again?\n",
    "        self.tt[2] = (cc[1] - cc[0])\n",
    "        for i in range(3):\n",
    "            self.ss[i] = np.linalg.norm(self.tt[i], ord=2)\n",
    "            self.tt[i] /= self.ss[i]\n",
    "            self.TT[i] = -3./4 * np.outer(self.tt[i], self.tt[i])\n",
    "            # HERE! if we don't use the hierarchical basis we need to add this, see...\n",
    "            self.TT[i] += 0.5*np.eye(2)\n",
    "            self.tt[i] *= -3./(2*self.ss[i])\n",
    "\n",
    "        self.M[self.rows[3], self.cols[2]] = self.tt[0].copy().reshape(2,1)\n",
    "        self.M[self.rows[3], self.cols[3]] = self.TT[0].copy()\n",
    "        self.M[self.rows[3], self.cols[4]] = -self.tt[0].copy().reshape(2,1)\n",
    "        self.M[self.rows[3], self.cols[5]] = self.TT[0].copy()\n",
    "\n",
    "        self.M[self.rows[4], self.cols[0]] = self.tt[1].copy().reshape(2,1)\n",
    "        self.M[self.rows[4], self.cols[1]] = self.TT[1].copy()\n",
    "        self.M[self.rows[4], self.cols[4]] = -self.tt[1].copy().reshape(2,1)\n",
    "        self.M[self.rows[4], self.cols[5]] = self.TT[1].copy()\n",
    "\n",
    "        self.M[self.rows[5], self.cols[0]] = self.tt[2].copy().reshape(2,1)\n",
    "        self.M[self.rows[5], self.cols[1]] = self.TT[2].copy()\n",
    "        self.M[self.rows[5], self.cols[2]] = -self.tt[2].copy().reshape(2,1)\n",
    "        self.M[self.rows[5], self.cols[3]] = self.TT[2].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now aggregate the previous cell-wise operator into one acting over discrete functions over the whole space.\n",
    "\n",
    "**FIXME:** the following code is very inefficient:\n",
    "\n",
    "1. There is no need to create and multiply by the first 6 rows of the cell matrix. If we didn't create a \"global matrix\", i.e. acting on whole vectors of coefficients, but instead proceeded cell by cell this would be faster (albeit at the cost of a python loop, which might prove to be even worse)\n",
    "2. When building such potentially large sparse matrices one should preinit the sparsity pattern.\n",
    "3. It should be possible to loop through all facets instead of cells. This should reduce computing time because of the fewer updates to the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     43
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dkt_gradient_operator(V, Vp):\n",
    "    \"\"\" Build sparse PETSc matrix of the discrete gradient\n",
    "    operator between V and Vp.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "        V: Source FunctionSpace of type DKT.\n",
    "        Vp: Target (Vector)FunctionSpace of type P2\n",
    "            (WITHOUT a hierarchical basis)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        Dh: PETSc sparse matrix such that for u in V,\n",
    "              Dh * u.vector().array()\n",
    "            yields the vector of coefficients of the discrete\n",
    "            gradient of u in Vp.\n",
    "    \"\"\"   \n",
    "    gdim = V.mesh().geometry().dim()\n",
    "    assert V.ufl_element().shortstr()[:3] == 'DKT',\\\n",
    "           \"I need a DKT source space\"\n",
    "    assert Vp.ufl_element().shortstr()[:14] == 'Vector<%d x CG2' % gdim,\\\n",
    "           \"I need a CG2 VectorFunctionSpace with %d components\" % gdim\n",
    "    assert Vp.mesh().num_cells() == V.mesh().num_cells(),\\\n",
    "           \"Number of cells in the meshes don't match\"\n",
    "    dm = V.dofmap()\n",
    "    dmp1, dmp2 = Vp.sub(0).dofmap(), Vp.sub(1).dofmap()\n",
    "    e = V.element()\n",
    "    ep = Vp.element()\n",
    "    tdim = e.topological_dimension()\n",
    "    assert tdim == 2, \"I need topological dimension 2 (was %d)\" % tdim\n",
    "    coords = V.tabulate_dof_coordinates().reshape((-1, tdim))\n",
    "\n",
    "    Dh = PETSc.Mat()\n",
    "    Dh.create(PETSc.COMM_WORLD)\n",
    "    Dh.setSizes([Vp.dim(), V.dim()])\n",
    "    Dh.setType(\"aij\")\n",
    "    Dh.setUp()\n",
    "    # In order not to assume the following it would be enough\n",
    "    # to create the array inside the loop...\n",
    "    warning(\"Assuming all cells have the same number of dofs\")\n",
    "\n",
    "    ## Preinit element-wise gradient\n",
    "    grad = DKTCellGradient()\n",
    "\n",
    "    # TODO: check this for parallel operation...\n",
    "    istart, iend = Dh.getOwnershipRange()\n",
    "    #print (\"This process owns the range %d - %d\" % (istart, iend))\n",
    "\n",
    "    for cell_id in range(V.mesh().num_cells()):\n",
    "        cell = Cell(V.mesh(), cell_id)\n",
    "        grad.update(cell)\n",
    "        # Massage dofs into the ordering we require with\n",
    "        # both coordinates for each vector-valued dof consecutive.\n",
    "        dest_rows = np.array(list(chain.from_iterable(zip(dmp1.cell_dofs(cell_id),\n",
    "                                                          dmp2.cell_dofs(cell_id)))),\n",
    "                             dtype=np.intc)\n",
    "        dest_rows = dest_rows[(dest_rows>=istart) & (dest_rows < iend)]\n",
    "        # dofs in V are already in the proper ordering (point eval, gradient eval, ...)\n",
    "        dest_columns = dm.cell_dofs(cell_id)\n",
    "\n",
    "        # NOTE: the copy() is necessary!\n",
    "        #print(\"Setting:\\n\\trows=%s\\n\\tcols=%s\\nwith:\\n%s\" % (dest_rows, dest_columns, M))\n",
    "        Dh.setValues(dest_rows, dest_columns, grad.M.copy())\n",
    "        assert len(dest_rows)*len(dest_columns) ==  grad.M.size, \"duh\"\n",
    "    Dh.assemble()\n",
    "    return Dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It remains only to add an interface to the previous computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_dkt_gradient(V, Vp, u):\n",
    "    \"\"\" Returns the discrete gradient of u.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "        V: Source FunctionSpace of type DKT. (redundant...)\n",
    "        Vp: Target VectorFunctionSpace of type CG2.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        A Function in Vp interpolating the gradient of u.\n",
    "    \"\"\"\n",
    "    Dh = dkt_gradient_operator(V, Vp)\n",
    "    du = Function(Vp)\n",
    "    petsc_u = as_backend_type(u.vector()).vec()\n",
    "    petsc_du = as_backend_type(du.vector()).vec()\n",
    "    \n",
    "    Dh.mult(petsc_u, petsc_du)\n",
    "    du.vector().apply('insert')\n",
    "\n",
    "    return du"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "from utils import ExpressionAD\n",
    "from interpolation import interpolate_hermite\n",
    "import mshr\n",
    "import autograd as ad\n",
    "\n",
    "domain = mshr.Rectangle(Point(1.0, 1.0), Point(2.0, 2.0))\n",
    "mesh = mshr.generate_mesh(domain, 10)\n",
    "W = FunctionSpace(mesh, 'DKT', 3)\n",
    "T = VectorFunctionSpace(mesh, 'Lagrange', 2, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fun(x,y):\n",
    "    return x*y**2 # 2*x**2*y/ad.numpy.sqrt(y)  # This breaks things\n",
    "f = ExpressionAD(fun=fun, degree=3, domain=mesh)\n",
    "w = interpolate_hermite(f, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%timeit  #(~24ms)\n",
    "gf = project(nabla_grad(f), T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%timeit  #(~237ms)\n",
    "dgf = compute_dkt_gradient(W, T, w)\n",
    "_ = plot(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not np.allclose(gf.vector().array(), dgf.vector().array(), atol=1e-11):\n",
    "    dgfa, gfa = dgf.vector().array(), gf.vector().array()\n",
    "    difs = np.abs(dgfa-gfa)/np.maximum(np.abs(dgfa), np.abs(gfa))\n",
    "    print(\"Results differ. Maximum difference = %.3e, mean relative difference = %.3e, std dev = %.3e\" \n",
    "          % (np.max(np.abs(dgfa-gfa)), difs.mean(), difs.std()))\n",
    "    pl.figure(figsize=(20,10))\n",
    "    pl.subplot(1,2,1)\n",
    "    plot(mesh, alpha=0.3)\n",
    "    plot(gf)\n",
    "    pl.title(\"Projection\")\n",
    "    pl.subplot(1,2,2)\n",
    "    plot(mesh, alpha=0.3)\n",
    "    plot(dgf)\n",
    "    _ = pl.title(\"Discrete operator\")\n",
    "else:\n",
    "    print(\"The gradients agree!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how values at the vertices seem to be correct, yet there is still a difference. The issue seems to be mostly at the facet dofs: To see this let us first extract both vertex and facet dofs for each subspace, then compute the list of dofs where the projected gradient and the one computed by the discrete operator disagree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dofs import plot_dofs, plot_field_at_dofs, extract_dofs_with_mask\n",
    "\n",
    "vertex_dofs_0 = extract_dofs_with_mask(T, [0,1,2])   # T.sub(0)\n",
    "vertex_dofs_1 = extract_dofs_with_mask(T, [6,7,8])   # T.sub(1)\n",
    "facet_dofs_0 = extract_dofs_with_mask(T, [3,4,5])    # T.sub(0)\n",
    "facet_dofs_1 = extract_dofs_with_mask(T, [9,10,11])  # T.sub(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bad_dofs(f:Function, g:Function, dofs0, dofs1, plot=True, eps=1e-6):\n",
    "    \"\"\"\n",
    "        f,g: Functions over the same VectorFunctionSpace V of dim=2\n",
    "        dofs0,\n",
    "        dofs1: dofs in V.sub(0), V.sub(1), i.e. they should be of the\n",
    "               same length and go in pairs\"\"\"\n",
    "    assert f.geometric_dimension() == g.geometric_dimension() == 2, \"Need 2d vector valued functions\"\n",
    "    assert f.function_space().element().topological_dimension() == 2, \"Need a 2d domain\"\n",
    "    vf = gf.vector().array()\n",
    "    vg = dgf.vector().array()\n",
    "    assert len(dofs0) == len(dofs1)\n",
    "    num_dofs = len(dofs0)\n",
    "    bad_dofs = []\n",
    "    for i,j in zip(dofs0, dofs1): # range(num_vertices):\n",
    "        try:\n",
    "            dx, dy = vg[i] / vf[i], vg[j] / vf[j]\n",
    "            if not (near(dx, 0, eps=eps) or near(dx, 1, eps=eps)) or\\\n",
    "               not (near(dy, 0, eps=eps), near(dy, 1, eps=eps)):\n",
    "                #print(\"Not colinear with factors = {},{}\".format(dx, dy))\n",
    "                bad_dofs.append(i)\n",
    "        except Exception as e:\n",
    "            print(\"{} Skipping {}\".format(e, i))\n",
    "    print(\"Computed gradients differ at %.2f%% of the dofs.\" % \n",
    "          (100*len(bad_dofs)/num_dofs))\n",
    "    if plot:\n",
    "        plot_dofs(T, bad_dofs, s=60, c='red', alpha=0.6)\n",
    "        plot_field_at_dofs(f, dofs0, dofs1, alpha=0.7, color='orange')\n",
    "        plot_field_at_dofs(g, dofs0, dofs1, alpha=0.8, color='green')\n",
    "    else:\n",
    "        return bad_dofs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are two nice plots. The red dots are placed at dofs where there is a discrepancy between both methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pl.figure(figsize=(24,12))\n",
    "pl.subplot(1,2,1)\n",
    "bad_dofs(gf, dgf, vertex_dofs_0, vertex_dofs_1, eps=1e-3)\n",
    "pl.title(\"Vertex dofs\")\n",
    "pl.subplot(1,2,2)\n",
    "bad_dofs(gf, dgf, facet_dofs_0, facet_dofs_1, eps=1e-3)\n",
    "_ = pl.title(\"Facet dofs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that we are basically ok, up to (sometimes gross) numerical errors (which **may build up!**, so we should be careful here...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating over facets\n",
    "\n",
    "As mentioned above, the construction of the global matrix for $\\nabla_h$ is extremely inefficient for several reasons. One is that we perform useless multiplications with the identity matrix, another that we go twice over all inner facets.\n",
    "\n",
    "FINISH THIS if the C++ implementation doesn't pan out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dofs import facets_dofmap\n",
    "\n",
    "dm = facets_dofmap(T)\n",
    "dm.facet_dofs[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C++ implementation\n",
    "\n",
    "The only sensible way to make the code above faster is to ditch python and integrate the computation of $\\nabla_h$ into the assembly process. A straightforward approach is to basically copy `dolfin/fem/Assembler.*` and adapt it to our problem.\n",
    "\n",
    "But first we need to compute the `DKTGradientOperator` local to a cell. Here follows a first naive implementation (lots of copying etc.):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c++\n",
    "  /// Computes the transformation matrix for $\\nabla_h$ over one cell.    \n",
    "  /// This assumes that the dofs of the \"source\" cell (in DKT) are ordered as\n",
    "  /// \n",
    "  ///      w1, w1_x, w1_y, w2, w2_x, w2_y, w3, w3_x, w3_y,\n",
    "  ///      \n",
    "  /// i.e. point evaluation, partial derivatives for each vertex in turn, and\n",
    "  /// those of the \"target\" cell (in P_2^2) as\n",
    "  /// \n",
    "  ///     t1_0, t1_1, t2_0, t2_1, t3_0, t3_1,\n",
    "  ///     ts1_0, ts1_1, ts2_0, ts2_1, ts3_0, ts3_1\n",
    "  ///     \n",
    "  /// i.e. values at the three vertices, then values at the the midpoints of\n",
    "  /// the opposite sides.\n",
    "  class DKTGradient\n",
    "  {\n",
    "    std::array<double, 3*2> tt;    // tangent vectors\n",
    "    std::array<double, 3*2*2> TT;  // tt_i x tt_i^T (tensor prods.)\n",
    "    std::array<double, 3> ss;      // cell side lengths\n",
    "    std::array<double, 12*9> M;    // gradient matrix\n",
    "    std::vector<double> cc;        // cell coordinates\n",
    "\n",
    "  public:\n",
    "    DKTGradient()\n",
    "    {\n",
    "      /// Initialise the local cell gradient matrix.\n",
    "      /// (interpolates from P_2^2 into DKT)\n",
    "      M.fill(0);  // Necessary?\n",
    "\n",
    "      // Fill identity submatrices\n",
    "      auto RC = [](size_t r, size_t c) -> size_t { return r*9 + c; };\n",
    "      M[RC(0,1)] = 1; M[RC(1,2)] = 1;\n",
    "      M[RC(2,4)] = 1; M[RC(3,5)] = 1;\n",
    "      M[RC(4,7)] = 1; M[RC(5,8)] = 1;\n",
    "    }\n",
    "    \n",
    "    /// Updates the operator matrix for the given Cell\n",
    "    void update(const Cell& cell)\n",
    "    {\n",
    "      // Vector and matrix access helpers for tt, TT and M\n",
    "      // respectively. I guess these will be optimized away...\n",
    "      auto IJ = [](size_t i, size_t j) -> size_t \n",
    "          { return i*2 + j; };\n",
    "      auto IIJ = [](size_t k, size_t i, size_t j) -> size_t \n",
    "          { return 4*k + i*2 + j; };\n",
    "      auto RC = [](size_t r, size_t c) -> size_t \n",
    "          { return r*9 + c; };\n",
    "\n",
    "      cell.get_vertex_coordinates(cc);\n",
    "      \n",
    "      // FIXME: shouldn't this depend on the orientation?\n",
    "      tt[IJ(0,0)] = cc[IJ(2,0)] - cc[IJ(1,0)];\n",
    "      tt[IJ(0,1)] = cc[IJ(2,1)] - cc[IJ(1,1)];      \n",
    "      tt[IJ(1,0)] = cc[IJ(2,0)] - cc[IJ(0,0)];  // Why is this negated?\n",
    "      tt[IJ(1,1)] = cc[IJ(2,1)] - cc[IJ(0,1)];\n",
    "      tt[IJ(2,0)] = cc[IJ(1,0)] - cc[IJ(0,0)];\n",
    "      tt[IJ(2,1)] = cc[IJ(1,1)] - cc[IJ(0,1)];\n",
    "      \n",
    "      auto outer = [&](double x0, double x1) -> std::array<double, 2*2> {\n",
    "        return { x0*x0, x0*x1, x1*x0, x1*x1 };\n",
    "      };\n",
    "      \n",
    "      for (int i=0; i < 3; ++i) {\n",
    "        ss[i] = std::sqrt(tt[IJ(i,0)]*tt[IJ(i,0)] + tt[IJ(i,1)]*tt[IJ(i,1)]);\n",
    "        tt[IJ(i,0)] /= ss[i];\n",
    "        tt[IJ(i,1)] /= ss[i];\n",
    "        auto m = outer(tt[IJ(i,0)], tt[IJ(i,1)]);\n",
    "        TT[IIJ(i,0,0)] = 0.5 - 0.75*m[IJ(0,0)];\n",
    "        TT[IIJ(i,0,1)] =     - 0.75*m[IJ(0,1)];\n",
    "        TT[IIJ(i,1,0)] =     - 0.75*m[IJ(1,0)];\n",
    "        TT[IIJ(i,1,1)] = 0.5 - 0.75*m[IJ(1,1)];\n",
    "        tt[IJ(i,0)] *= -3/(2*ss[i]);\n",
    "      }\n",
    "\n",
    "      // Copy onto the gradient (sub) matrix \n",
    "      // This is quite wasteful...\n",
    "      auto copytt = [&](size_t i, size_t r, size_t c) {\n",
    "        M[RC(r,c)] = tt[IJ(i,0)]; M[RC(r+1,c)] = tt[IJ(i,1)];\n",
    "      };\n",
    "      auto copy_tt = [&](size_t i, size_t r, size_t c) {\n",
    "        M[RC(r,c)] = -tt[IJ(i,0)]; M[RC(r+1,c)] = -tt[IJ(i,1)];\n",
    "      };\n",
    "      auto copyTT = [&](size_t i, size_t r, size_t c) {\n",
    "        M[RC(r,c)] = TT[IIJ(i,0,0)]; M[RC(r,c+1)] = TT[IIJ(i,0,1)];\n",
    "        M[RC(r+1,c)] = TT[IIJ(i,1,0)]; M[RC(r+1,c+1)] = TT[IIJ(i,1,1)];\n",
    "      };\n",
    "\n",
    "      copytt(0, 3, 2);\n",
    "      copyTT(0, 3, 3);\n",
    "      copy_tt(0, 3, 4);\n",
    "      copyTT(0, 3, 5);\n",
    "      \n",
    "      copytt(0, 4, 0);\n",
    "      copyTT(0, 4, 1);\n",
    "      copy_tt(0, 4, 4);\n",
    "      copyTT(0, 4, 5);\n",
    "      \n",
    "      copytt(0, 5, 0);\n",
    "      copyTT(0, 5, 1);\n",
    "      copy_tt(0, 5, 2);\n",
    "      copyTT(0, 5, 3);\n",
    "    }\n",
    "\n",
    "    /// Transform local dof coefficients from P_3^red to P_2^2\n",
    "    void apply(const std::vector<double>& p3coeffs, \n",
    "               std::vector<double>& p22coeffs)\n",
    "    {\n",
    "      // Copy coefficients for partial derivatives\n",
    "      p22coeffs[0] = p3coeffs[1];  p22coeffs[1] = p3coeffs[2];\n",
    "      p22coeffs[2] = p3coeffs[4];  p22coeffs[3] = p3coeffs[5];\n",
    "      p22coeffs[4] = p3coeffs[7];  p22coeffs[5] = p3coeffs[8];\n",
    "\n",
    "      // Matrix * vec multiplication\n",
    "      for (auto i = 0; i < 6; ++i) {\n",
    "        p22coeffs[6+i] = 0;\n",
    "        for (auto j = 0; j < 9; ++j) {\n",
    "           p22coeffs[6+i] += M[i*9+j]*p3coeffs[j];\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once that works we use it in the assembler. We need to be careful with:\n",
    "\n",
    "1. Combining the dofmaps from two forms: this shouldn't be that hard. We compute the local tensor for the P2 form then compute $M^T A M$ and apply the local-to-global dofmap for the DKT space in order to populate the global tensor.\n",
    "\n",
    "2. Precomputing the sparsity pattern. This is done in `AssemblerBase::init_global_tensor(GenericTensor& A, const Form& a)` and relies on the FunctionSpaces associated to the Form being assembled, so we'd need to rewrite this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dolfin import *\n",
    "import numpy\n",
    "import os\n",
    "\n",
    "with open(\"KirchhoffAssembler.h\", \"r\") as fd:\n",
    "    code = fd.read()\n",
    "    ka = compile_extension_module(code=code,\n",
    "                                  source_directory=\".\",\n",
    "                                  sources=[\"KirchhoffAssembler.cpp\"],\n",
    "                                  include_dirs=[os.path.abspath(\".\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump\n",
    "\n",
    "A quick retesting of the assembly of the matrix for $\\nabla_h$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mesh = UnitSquareMesh(1,1)\n",
    "W = FunctionSpace(mesh, 'DKT', 3)\n",
    "T = VectorFunctionSpace(mesh, 'Lagrange', 2, dim=2)\n",
    "\n",
    "w = Function(W)\n",
    "dmw = W.dofmap()\n",
    "wdofs_cell0 = dmw.cell_dofs(0)\n",
    "wdofs_cell1 = dmw.cell_dofs(1)\n",
    "new_vals = np.zeros_like(w.vector().array())\n",
    "new_vals[wdofs_cell0] = [2,1,1,2,1,1,2,1,1]\n",
    "w.vector().set_local(new_vals)\n",
    "\n",
    "\n",
    "Dh = DKTCellGradient(T.element().space_dimension(), W.element().space_dimension())\n",
    "Dh.update(Cell(W.mesh(), 0))\n",
    "product = Dh.M@w.vector().array()[wdofs_cell0]\n",
    "\n",
    "dw2 = Function(T)\n",
    "dm0 = T.sub(0).dofmap()\n",
    "dm1 = T.sub(1).dofmap()\n",
    "tdofs0_cell0 = dm0.cell_dofs(0)\n",
    "tdofs1_cell0 = dm1.cell_dofs(0)\n",
    "#### <strike>DON'T FIXME!!!! this is NOT WRONG:</strike>\n",
    "dest_rows = np.array(list(chain.from_iterable(zip(tdofs0_cell0,tdofs1_cell0))), dtype=np.intc)\n",
    "print(tdofs0_cell0, tdofs1_cell0)\n",
    "print(dest_rows)\n",
    "#### found manually:\n",
    "dest_rows = [2, 3, 12, 13, 4, 5, 14, 15, 6, 7, 16, 17]\n",
    "new_vals2 = np.zeros_like(dw2.vector().array())\n",
    "new_vals2[dest_rows] = product\n",
    "dw2.vector().set_local(new_vals2)\n",
    "print(product)\n",
    "print(Dh.M)\n",
    "\n",
    "pl.figure(figsize=(12,6))\n",
    "plot(T.mesh(), alpha=0.3)\n",
    "vertex_dofs_0 = extract_dofs_with_mask(T, [0,1,2])   # T.sub(0)\n",
    "vertex_dofs_1 = extract_dofs_with_mask(T, [6,7,8])   # T.sub(1)\n",
    "facet_dofs_0 = extract_dofs_with_mask(T, [3,4,5])    # T.sub(0)\n",
    "facet_dofs_1 = extract_dofs_with_mask(T, [9,10,11])  # T.sub(1)\n",
    "plot_field_at_dofs(dw2, vertex_dofs_0, vertex_dofs_1, color='orange')\n",
    "plot_field_at_dofs(dw2, facet_dofs_0, facet_dofs_1, color='green')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "167px",
    "width": "254px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "1247px",
    "left": "0px",
    "right": "1601px",
    "top": "84px",
    "width": "368px"
   },
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
